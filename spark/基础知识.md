md语法
    # 代表一级标题
    ## 代表二级标题

spark：  
    spark-core为spark核心组件  
    spark-sql为与hadoopf交互的组件  
    spark-streaming为流式处理组件  
    spark-mlib: 机器学习组件  
    spark-XGriph: 图处理组件  

## spark-sql：
    新建交互对象： val spark = sparkSession.builder().appName()


> ### DataFrame操作 
    > + 1、show() : 显示所有数据
    > + 2、collect: 获取全部数据并放入一个Array中
    > + 3、describe(cols): 获取指定字段的统计信息
        
        支持
            DataFrame.where(条件)筛选
            DataFrame.filter(条件)
            DataFrame.select(指定字段)
            DataFrame.selectExpr("user", "take as visittype", "to_date(visittime)").show()
            DataFrame.orderBy("clos").show(false)
            DataFrame.drop("name") 去除指定字段
            DataFrame.groupBy("name") 返回的类型为GroupedData类型，不能直接用show来进行展示，通常还需要跟一些分组统计函数，
                - max（colName） 字段最大值
                - min(colName) 字段最小值
                - mean(colName) 字段平均值
                - sum(colName) 指定字段和
                - count() 元素个数

            DataFrame.dropDuplicates: 根据指定字段去重
                如: df.dropDuplicates("user", "type").show()
            DataFrame.agg() 实现聚合操作, 一般需配合groupBy方法使用
                如 df.agg("id"->"max", "user"->"sum").show()
# RDD 和 DataFrame 和 DataSet  

    发展历史  

        2011年Rdd  
        2013年 DataFrame  
        2015年 DataSet  

    Rdd: 
        - 不可变分布式元素集合，在集群中跨节点分布，可以通过若干提供了转换和处理的底层API进行并行处理
        - 使用场景
            - 对数据集进行最基本的转换、处理和控制
            - 数据时非结构化的
            - 可以通过函数式编程而非特定领域内的表达来处理数据
            - 不需要过高的性能
        - 优点
            - 强大，内置很多操作函数
            - 面向对象，直接存储Java对象，类型转化安全
        - 缺点
            - 基本和hadoop一样万能，无针对特殊场景的优化，比如对于结构化数据处理想对于sql会比较麻烦
            - 默认采用java序列号方式，序列化结果较大，数据存储在堆内存，GC比较频繁

    DataFrame
        - 以RDD为基础的分布式数据集，类似于传统数据库中的二维表格，并且引入了schema
        - 同RDD的区别
        - DataFrame的数据集按指定咧存储，即结构化数据，类似于创通数据哭中的表
        - 优点
          - 结构化数据处理方便，支持kv数据,也支持Hive Table  和 Mysql等传统数据表
          - 有针对性优化，由于数据结构源信息spark已经保存，序列化不需要带上元信息，数据保存在堆外内存，减少了gc次数
          - hive兼容，支持hql, udf等
        - 缺点
          - 编译时不能类型转化安全检查，运行时才能确定是否有问题
          - 对于对象支持不友好，rdd内部数据直接以java对象存储，dataFrame内存存储的是Row对象而非自定义对象

    DataSet
        - 强类型，特定领域的对象，这种对象可以进行函数式或者关系操作并行的进行转换
        - 每个Dataset都有一个恒威DataFrame的非类型化视图，这个视图时行的数据集，即DataSet(Row)
        - 只在执行action操作时计算
        - 与RDD的主要区别
            - Dataset时特定域的对象集合，而Rdd时任何对象的集合
            - Dataset的API总是强类型的
        - 优点
            - 整合了rdd和DataFrame的优点，支持结构化和非结构化数据
            - 同rdd一样，支持自定义对象存储
            - 和dataFrame一样，支持结构化数据sql查询
            - 采用堆外内存存储，gc友好
            - 类型转化安全，代码友好
            - 官方建议使用dataSet

            

# Spark-Streaming
    - spaark-Streaming时Spark核心API的一个拓展，对流式数据具有可拓展性、高吞吐量，可容错性等特点
    - 可以从kafka, flume, Twitter等源获取数据，也可以通过由高阶函数map、reduce、join等组成的复杂算法计算出数据
    - 最后将数据推送到文件系统、数据库、实时仪表盘中。
    - 更多的是将处理后的数据应用到机器学习算法、图处理算法中

    工作原理  

    Spark-Streaming接受实时的输入数据流，然后将这些数据切分为batch数据供Spark引擎处理，Spark引擎生成最终处理数据

    Spark支持一个高层的抽象，称为离散流(Discretized Stream)，或者称为Dstream
    Dstream支持从kafka, flume的目光源获取数据创建，在内部，Dstream是一系列的Rdds组成

    spark-streaming使用

    streamingContext是spark所有流操作的主要入口

    基本概念
    kafka : spark-steaming-kafka
    flume : spark-streaming-flume

<big>初始化</big>

        为了初始化sparkStreaming,必须创建一个StreamingContext.

        import org.apache.spark._
        import org.apache.spark.streaming._

        val conf = new SparkConf().setAppName(appName).setMaster(master)
        val ssc = new StreamingContext(conf, Seconds(1))

        其中
            master为集群url或者本机Local[*](表示在本机执行spark程序)
            appName为任务在集群上显示的名称

        当context确定后，必须按照已下步骤进行：
            - 定义输入数据源
            - 准备好流计算指令
            - 利用streamingContext,start()方法接受和处理数据
            - 处理过程将一直持续，直到streamingContext.stop()方法被调用

        注意
            1、一旦一个context启动，就不能有新的流算子简历或者添加到context
            2、context停止后，无法重新启动
            3、在jvm中，同一时间只能有一个StreamingContext处于活跃状态
            4、在StreamingContext上调用stop()方法，也会关闭sparkContext对象，如果仅仅想值关闭StreamingContext对象，可以写stop(false)
            5、一个sparkContext对象可以重复利用去创建多个StreamingContext对象，前提是前一个streamingContext必须关闭
    
<big> 离散流 </big>   
        
        Dstream(离散流)
            - 是sparkStreaming提供的基本抽象
            - 代表一个连续的数据流
            - 在内部是一些列连续RDD组成
            - 其中每个RDD都包含确定时间间隔内的数据 RDD @ time1
            - 对Dstreams的操作都可以转化为对Dstream中隐含RDD的操作

        输入Dstreams
            - 表示从数据源获取输入数据流的Dstreams
            - 每一个Dstream都与一个Receiver对象相关联
            - 两类数据源
              - 基本源(Basic sources): 在StreamingContext API中可直接使用
              - 高级源(Advanced sources)：如kafka, flume需要使用额外的类来操作

        receivers
            - 从数据源中获取数据，并存入内存中用于处理
            - 是一个长期运行的任务运行在worker或者excutor中，长期占用一个核
            - 如果分配给应用程序的核的数量少于或者等于输入Dstreams或者receivers的数量，系统只能够接受数据而不能处理他们
            - 当运行在本地时，如果masterURL设置成了local,这样只有一个核运行任务，这样对于应用程序是不足的，因为作为receiver的输入Dstream将会占用这个核，这样就没有剩余的核来处理数据了。

        Dstreams中的转换操作
            - 绝大多数对于RDD的操作都可以用于对于Dstream数据进行操作
            - 特别的有两个算子 
              - updateStateByKey
                - 该操作允许不断用新信息更新他的同事保持任意状态
                - 操作需要两个步骤 1)定义状态-状态可以是任意数据类型， 2)定义状态更新函数
              - transform
                - 允许Dstream运行任意的RDD2RDD函数。
                - 能够被用来应用任何没在DstreamAPI中提供的RDD操作
                - 也可以在transfoirm方法中用机器学习核图计算算法
            - 窗口(window)操作
              - 窗口是指一个时间区间内的数据
              - 将Origin RDDs --> Windowed Dstream
              - 任何一个窗口操作迪欧需要两个参数
                - 窗口长度： 窗口持续时间
                - 滑动的时间间隔： 窗口操作执行的时间间隔
                - 这两个参数必须是Dstream的批时间间隔的倍数

        Dstream上的输出操作
            - 输出操作允许Dstream的操作推到如数据库，文件系统等外部系统中，实际触发的是Dstream的转换操作
            - foreachEDD的设计模式
              - dstream.foreachRDD是一个强大的原语
              - 输出方式通过懒执行来操作DstreamRDD
              - 默认情况下，Dstream输出操作是分时进行的，应该按照应用程序的定义顺序按序执行
  
        缓存与持久化
            - 和Rdd类似，Dstreams允许开发者持久化流数据到内存中
            - 在Dstream上使用persist() 可以自动的持久化Dstream中的RDD到内存中，
            - reduceByWindow核reduceByKeyAndWindow、updateStateByKey持久化是默认的，不需要手动调用Persist()
            - 通过kafka、flume获取的输入流，默认的持久化策略是复制数据到两个不同的节点以容错。

        checkpointing
            - 流程序必须全天运行，必须能够解决应用程序逻辑无关的故障。因此spark需要checkpoint来保存足够的信息到容错系统
            - metadata checkpointing:
              - 保存流计算的定义信息到容错存储系统中，元数据包括
                - configuration: 创建spark streaming应用程序的配置信息
                - Dstream options: 定义Streaming应用程序的操作集合
                - Incomplete batches: 操作存在队列中的未完成的批
            - Data checkpointing
              - 保存生成的RDD到可靠的存储系统中，这在有状态transformation中是必须的。
            - 元数据checkpoint主要是为了从driver故障中恢复数据
            - 设置checkpointing
              - 可以通过streamingContext.checkpoint(checkpointDirectory)方法来做
              - 如果想从driver中恢复数据应该以以下集中方式
                - 1、当程序是第一次启动时，新建一个StreamingContext，启动所有stream,调用start()方法
                - 2、当程序因为故障重新启动，他将会从checkpoint目录重新创建StreamingContext

## sparkContext
    sparkContext是spark的配置类
    用于连接spark集群，创建rdd ,广播变量等
    配置以键值对的形式存在
    
    广播变量: 有点类似与全局变量，使用广播变量后，每个excutor中只会有driver中的一个副本

## collectAsMap
    将tuple中的元组转化为dict









    

    








