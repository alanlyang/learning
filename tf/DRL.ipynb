{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1600076823225",
   "display_name": "Python 3.7.3 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow import data as tfdata\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow import losses\n",
    "from tensorflow.keras import initializers as init\n",
    "\n",
    "from tensorflow.keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DRL:  \n",
    "    - 全称(Deep Reinforcement Learning) 深度强化学习  \n",
    "    - 强调基于环境行动，以取得最大化预期利益  \n",
    "    - alphaGO即是深度强化学习的典型应用  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里使用**openAI** 提供的环境库 gym    \n",
    "gym 主要由三个函数\n",
    "- make\n",
    "- render\n",
    "- env\n",
    "\n",
    "游戏中的两大引擎为物理引擎和图像引擎，gym中的render主要是负责图形的渲染，make则主要是物理引擎\n",
    "\n",
    "这里我们使用深度强化学习玩cartPole(倒立摆)游戏。倒立摆是控制论中的经典问题，在这个游戏中，一根杆的底部通过轴与小车相连组成一个不稳定系统。  \n",
    "我们需要通过控制小车左右移动，以使得杆一直保持垂直平衡状态"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "与gym的交互类似一个回合游戏:\n",
    "- 首先获取游戏厨师状态\n",
    "- 每个回合t中，选择一个动作交由gym执行\n",
    "- gym 返回下一个状态和当前回合的收益值\n",
    "- 持续迭代过程，直到游戏终止\n",
    "\n",
    "这里使用深度强化学习中国的Deep Q-Learning方法来训练模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义超参"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import  gym\n",
    "# deque类似 list ,能够快速在两端添加或弹出\n",
    "from collections import deque\n",
    "\n",
    "#  训练总episodes\n",
    "num_episodes = 500\n",
    "# 探索过程所占的episode数量\n",
    "num_exploration_episodes = 100\n",
    "# 每个episode的最大回合数\n",
    "max_len_episode = 1000\n",
    "# 批次大小\n",
    "batch_size = 32\n",
    "# 学习率\n",
    "learning_rate = 0.001\n",
    "# 折扣因子\n",
    "gamma = 1\n",
    "# 探索起始时的探索率\n",
    "initial_epsilon = 1.\n",
    "# 探索终止时的探索率\n",
    "final_epsilon = 0.01\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义模型\n",
    "这里使用tf.kera.Model建立一个Q网络函数（Q-netword）,用于拟合Q learning中的Q函数   \n",
    "这里我们使用较简单的全连接神经网络进行拟合，该网络输入当前状态，输出各个动作下的Q-value(CartPole下为2维)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetWork(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.dense1 = tf.keras.layers.Dense(units=24, activation=tf.nn.relu)\n",
    "        self.dense2 = tf.keras.layers.Dense(units=24, activation=tf.nn.relu)\n",
    "        self.dense3 = tf.keras.layers.Dense(units=2)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.dense1(inputs)\n",
    "        x = self.dense2(x)\n",
    "        x = self.dense3(x)\n",
    "        return x\n",
    "    \n",
    "    def predict(self, inputs):\n",
    "        q_values = self(inputs)\n",
    "        return  tf.argmax(q_values, axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实例化一个游戏环境，参数为游戏名称\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "# 实例化模型对象\n",
    "model = QNetWork()\n",
    "# 定义优化器\n",
    "optimizer = optimizers.Adam(learning_rate=learning_rate)\n",
    "# 使用deque作为Q Learning的经验回放池\n",
    "replay_buffer = deque(maxlen=10000)\n",
    "\n",
    "epsilon = initial_epsilon\n",
    "# 开始探索迭代过程\n",
    "for episode_id in range(num_episodes):\n",
    "    # 初始化环境，获取初始状态\n",
    "    state = env.reset()\n",
    "    # 计算当前的探索率\n",
    "    epsilon = max(\n",
    "        initial_epsilon*(num_exploration_episodes - episode_id)/num_exploration_episodes,\n",
    "        final_epsilon\n",
    "    )\n",
    "\n",
    "    for t in range(max_len_episode):\n",
    "        # 对当前帧进行渲染\n",
    "        env.render()\n",
    "        # epsilon-greedy探索策略，以epsilon的概率选择随机动作\n",
    "        if random.random() < epsilon:\n",
    "            # 选择随机动作\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            # 模型计算出Q value最大的动作\n",
    "            action = model.predict(np.expand_dims(state, axis=0)).numpy()\n",
    "            action = action[0]\n",
    "        \n",
    "        # 让环境执行动作，或者执行玩动作的下一个状态，动作的奖励，游戏是否结束一级额外信息\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        # 如果游戏结束，则给予最大负奖励\n",
    "        reward = -10. if done else reward\n",
    "        # 将(state, action ,reward, next_state)放入经验回放池\n",
    "        replay_buffer.append((state, action, reward, next_state, 1 if done else 0))\n",
    "        # 更新state\n",
    "        state = next_state\n",
    "\n",
    "        # 游戏结束则退出本轮循环，进入下一个episode\n",
    "        if done:\n",
    "            print('episode %d \\t epsilon %f \\t score %d' %(episode_id, epsilon, t))\n",
    "            break\n",
    "        \n",
    "        if len(replay_buffer) >= batch_size:\n",
    "            # 从经验回放池中随机取一个批次的四元组，分别转化维Numpy数组\n",
    "            batch_state, batch_action, batch_reward, batch_next_state, batch_done = zip(\n",
    "                    *random.sample(replay_buffer, batch_size))\n",
    "            batch_state, batch_reward, batch_next_state, batch_done = \\\n",
    "                    [np.array(a, dtype=np.float32) for a in [batch_state, batch_reward, batch_next_state, batch_done]]\n",
    "            batch_action = np.array(batch_action, dtype=np.int32)\n",
    "\n",
    "            q_value = model(batch_next_state)\n",
    "            # reduce_max. 找出最大值\n",
    "            # 计算y\n",
    "            y = batch_reward + (gamma * tf.reduce_max(q_value, axis=1)) * (1 - batch_done)\n",
    "\n",
    "            with tf.GradientTape() as tape:\n",
    "                # 采用MSE来最小化距离\n",
    "                loss = losses.mean_squared_error(\n",
    "                    y_true=y,\n",
    "                    y_pred=tf.reduce_sum(model(batch_state)*tf.one_hot(batch_action, depth=2), axis=1)\n",
    "                )\n",
    "            grads = tape.gradient(loss, model.variables)\n",
    "            optimizer.apply_gradients(grads_and_vars=zip(grads, model.variables))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}