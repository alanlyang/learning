## keras Sequential/ Functional API 模式建立模型

    - Sequential
        当需要建立一个结构相对简单和典型的神经网络，如（MLP,CNN）等时，可以使用keras提供的api来进行模型的构建。
        最常用的神经网络结构时将一堆层按特定顺序堆叠起来，keras提供了
            tf.kears.Sequential() 来提供一个层的列表，来快速的建立一个tf.keras,Model模型并返回
        例如：
        model = tf.kears.Sequential(
            [
                tf.keras.layers.Flatten(),
                tf.keras.layers.Dense(100, activation=tf.nn.relu)
            ]
        )
        这种方式不能表示任意的神经网络结构（只能从上往下顺序堆叠）。

    - Functional API（）
        - 为了解决Sequential不能建立复杂模型的问题
        - 例如： 多输入/输出模型、 存在参数共享的模型
        - 使用方法
          - 将层作为可调用对象
          - 返回tensor
          - 将输入tensor和输出tensor提供给 tf.keras.Model的inputs 和 outputs
        - 模型建立完成后，通过tf.keras.Model和compile方法进行训练配置
          - compile接受三个参数
            - optimizer 优化器
            - loss 损失函数
            - metrics 评估指标
        - 最后使用tf.keras.Model的fit方法进行训练
          - fit 接受5个参数
            - x 训练数据
            - y lable
            - epochs 迭代次数
            - batch_size 批次的大小
            - validation_data 验证数据
        - 最后使用evaluate评估训练效果
          - evaluate接受两个参数
            - 测试数据
            - 测试label
    

## Flatten层
    用来将多维的数据一维化，将数据'压平'， 常用在"卷积层到全链接层的过度"
    Flatten不影响batch的大小

## Initializers
    初始化器用来设置keras隔层权重随机初始值的方法
    用来将初始化器传入keras层的参数名具体取决于具体的层。通常关键字为kernel_initializer和bias_initializer

    例如:
        model.add(Dense(64, kernel_initialize=tf.keras.initializer.RandomNormal))
    
    常用initializer
        - 1、Zeros() 将张量初始值设置为0的初始化器
        - 2、Ones()  将张亮初始值设置为1的初始化器
        - 3、Constant() 将tensor设置为一个常数
        - 4、RandomNormal()按照正态分布生成随机张亮的初始化器
            - mean: 随机值的平均数
            - stddev: 平均数的标准差
            - seed：随机数的种子
        - 5、RandomUniform() 按照均匀分布生成随机量的初始化器
        - 6、TruncatedNormal() 按照截尾正太分布生成随机tensor, 通常用来生成神经网络权重和滤波器的推荐初始化器
        - 7、Identity: 生成单位矩阵的初始化器，仅用于2D方阵
  
## from_tensor_slice
    - 在tf.data.Dataset中, Dataset API主要用于数据读取，构建输入数据的Pipline
    - 返回一个Dataset对象
    - 参数为tensors
      - 每一个都在第0维具有相同的大小
      - 是tensor的嵌套结构
    - 返回除下第一维的其他数据

## tf.GradientTape
    用处： 用于记录模型训练过程中所有可调节变量的梯度值（tape为胶带的意思）
    常用方法： 
        watch(tensor): 确保某个tensor能够被tape追踪
        gradient(target, sources, output_gradients=None)
            target: 被微分的tensor或者tensor列表
            source: tensors列表

            返回一个列表，表示各个变量的梯度值，和source中的变量列表一一对应

## apply_gradients(grad_and_vars, name=None)
    用处： 把计算出来的梯度更新到变量上
    参数： grads_and_vars:(gradient, variable)对应的列表
          name: 操作名
          


## keras自定义层/损失函数/评估指标
    事实上，不仅可以通过继承tf.keras.Model来自定义模型类，还可以通过继承tf.keras.layers.Layer编写自定义层

    自定义层：
      - 继承 tf.keras.layers.Layer类
      - 重写 __init__、build、call三个方法
      - 见自定义.ipynb

## tf.image.crop_to_bounding_box
    将图像裁剪到指定边界框
    返回图像的左上角位于image的offset_height, offset_width，右下角位于offset_height+target_height, offset_width+target_weidth

    参数:
        image: 形状为\[batch, height, wdith, channel] 的4D张量，或者形状为[height, width, channels]的3D张量 
        offset_height: 输入中结果左上角的垂直坐标
        offset_width: 输入中结果左上角的水平坐标
        target_height: 结果高度
        target_width: 结果宽度

## tf.stack()
    矩阵拼接函数，

## tf.unstack()
    矩阵的分解函数

## tf.name_scope
    定义命名空间，允许相同的变量名，便于复杂结构的分层

## tf.summary.scalar
    用来显示标量信息，通常用于画loss和auc