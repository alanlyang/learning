## 前馈神经网络
    定义：
        每个神经元只与前一层的神经元相连，接受前一层的输出，并且输入给下一层，隔层之间没有反馈
        全称 Feedforward Neural Netword, FNN, 简称前馈网络
        采用单向多层结果，整个网络无反馈，信号从输入层向输出层单向传播
    特点：
        网络结构简单，应用广泛
        能够一任意精度逼近任意连续函数及平方可积函数
        可以精确实现任意有限训练样本集

        从系统的观点：
        前馈网络是一种静态非线性映射， 通过简单的非线性处理单元的复合映射，可以 获得复杂的非线性处理能力
        从计算的观点：
        缺乏 丰富的动力学行为

        大部分前馈网络都是学习网络，其分类能力和模式识别能力一般都强于反馈网络
    
    常见前馈神经网络
        感知机网络：主要用于模式分类，也可以用在基于模式分类的学习控制和多模态控制中
        BP网络： 连接权调整使用反向传播学习算法的前馈网络，BP网络的神经元变化函数采用的Sigmoid函数，输出量是0-1之间的连续量，可实现输入到输出的任意非线性映射

## 全链接层： 线性变化 + 激活函数
    - 最常用和基础的层之一
    - 对输入矩阵A进行线性变化f(AW+b) + 激活函数的操作，f即为激活函数。如果不指定激活函数，则为纯线性变化
    - tf.keras.layers.Dense  参数如下
      - units L 输出tensor的维度
      - activation 激活函数
      - use_bias 是否使用偏置向量
      - kernel_initializer 和 bias_initializer 权重初始化器, 默认为glorot_uniform（截取的正态分布）
## 池化层
    池化层的目的：
        - 降低信息冗余
        - 提升模型的尺度不变形，旋转不变性
        - 防止过拟合
    常见类型
        - 最大池化池: 能够学习到图像的边缘和文立
        - 均值池化： 常见于SE模块以及分类模块中，优点在于可以减少估计均值的偏移，提升鲁棒性
        - 随机池化: 随机位置池化集合了随机池化和最大值池化
        - 中值池化： 基本很少见，参考的是图像处理中的中值滤波而引申的一中呢池化方式
        - 组合池化:
        - 分阶数池化： 见于pytorch
    简单解释：
        - 用某个区域内值的函数值来代替该区域
        - 如最大池化就是用区域内的最大值来代替该区域
    其他：
        在resNet之后，池化层在分类网络中应用主键变少，往往采用stride-2的卷积代替最大池化层

## 卷积层
    卷积层会对图像进行高维特征提取，使用的原理是数学中的卷积运算
    在tf.keras中主要使用conv2D实现。接受参数：
        - filters: 卷积层神经元数目
        - padding： same/valid,
          - 如果不使用padding,
              - 则每次卷积操作以后，图像都会变小，导致多次卷积后特征丢失
              - 边缘信息只被少量的卷积输出使用，导致许多边缘位置信息丢失
          - 使用valid,则卷积不进行边缘填充，图像经过卷积后会缩小
          - 使用same,则对边缘进行填充，默认填0
        - kernel_size： 感受野大小，
        - activation

## VGG16 网络
    - VGG 系列是由 Oxford 的Visual Geometry Group 提出的
    - 主要证明了增加网络深度能够在一定程度上影响网络的最终性能
    - 原理：
      - 和 AlexNet 相比，采用连续的几个3*3的卷积核代替AlexNet中的较大卷积核
      - 对于给定的感受野，采用堆积的小卷积核优于采用大的卷积核。多层非线性层可以增加网络深度来保证学习更复杂的模式，而且参数更少
    - 网络结构
      - VGG16 包含16个隐层（13个卷积层，3个全链接层）
      - VGG19 包含19个隐层（16个卷积层，3个全链接层）
      - VGG 网络从头到尾都是使用的3*3的卷积核和2*2的max pooling
    - 优缺点
      - 优点：
        -  结构简洁，整个网络都使用同样大小的卷积核尺寸(3*3)和最大池化层尺寸(2*2)
        -  几个小滤波器卷积层的组合比一个大滤波器卷积层号
        -  验证了通过不断加深网络结构可以提高性能
      - 缺点
        -  耗费更多的计算资源，并且使用了更多的参数，导致了很高的内存占用

## RNN（循环神经网络）

    - 全称 Recurrent Neural Netword
    - cnn和dense都只能处理单一一次样本的输入，学习单个样本内的信息，无法学习前一个输入与后一个输入之间的关系，因此出现了RNN
    - RNN中state其实是上个时刻隐层的权重矩阵
    - RNN和传统神经网络的主要却别就在于每次都会将前一次的输出结果带到下一次的隐层中，一起训练
    - RNN 是一个链式结构，每个时间片使用的是相同的参数
    - RNN 的输出仅由权值，偏置、激活 函数确定
    - RNN的缺点也比较明显，短期记忆的影响比较大，长期的记忆影响就很小
      - RNN有短期记忆问题，无法处理很长的输入序列
      - 训练RNN需要投入大量的成本，训练收敛较慢
      - RNN模型训练中经常会出现梯度消失或者梯度爆炸，这是由RNN的权重矩阵循环相乘导致，相同函数的多次组合会导致极端的非线性行为

## LSTM(长短期记忆网络)
    - 参考资料： https://easyai.tech/ai-definition/lstm/
    - 全称： Long Short Term Memory
    - 是RNN的改进版，具有记忆长短期信息的能力,是一种特殊的RNN
    - 相比于RNN， LSTM选择只保留重要信息
    - LSTM引入了门(gate)机制用于控制特征的流通和损失
    - LSTM由一系列的LSTMCell（LSTM unit组成）
    - 所有的递归神经网络都具有神经网络的链式重复模块，在标准RNN中，这个重复模块具有非常简单的结构，只有单个的tanh层
    - LSTM也具有类似的链式结构，但是重复模块具有不同的结构，不是一个单独的神经网络层，而是四个，并且以非常特殊的形式进行交互
  <img src="https://easy-ai.oss-cn-shanghai.aliyuncs.com/2019-07-05-lstm.png">
    - LSTM的核心部分是图中最上面的一条线，一般称为cell state, 贯穿整个LSTM链式系统
      - C(t) = 遗忘门*C(t-1) + 输入门* 单元状态更新值
      - 遗忘门是一个向量，向量的每个元素都在[0, 1]之间
      - LSTM最重要的门机制表示 遗忘门和C(t-1)之间的单位乘关系
      - 输入门i(t)也是一个向量，元素也都在[0,1]之间，经由sigmod计算而成
      - 单元状态更新值由tanh计算而成
      - 输出门 = o(t)*tanh(C(t))
    - 总结起来一个LSTMCell 由输入门、遗忘门，输出门，状态更新组成
  
## GRU
    - 全称 Gated Recurrent Unit 
    - 是LSTM的一个变种，主要在LSTM的基础上做了一些简化和调整，在训练集较大的情况下可以节省很多时间
    - GRU在LSTM的基础上，将忘记门和输入门合成了一个单一的更新门
      - 同样还混合了细胞状态和隐藏状态。
      - 比LSTM的模型要简单，也是非常流行的变体
  <img src="https://pic4.zhimg.com/80/v2-1838ebd696f1e4d16e41f1a126ff85a0_720w.jpg">
  
    标准LSTM和GRU的差别不大，但是都比tanh要明显好很多，在选择标准LSTM或者GRU的时候还要看具体的任务是什么
    使用LSTM的原因是解决RNN Deep Netword的Gradient错误累计太多，以至于梯度爆炸或者梯度消失
    GRU的构造比LSTM少一个gate， 因此计算时少几个矩阵相乘，在训练数据很大的情况下GRU能节省很多时间

## BN网络层
    - 全称 Batch Normalization 批量标准化
    - 主要是为了解决梯度消失的问题
    - 计算步骤如下：
        - 1、输入数据
        - 2、计算输入数据的均值
        - 3、计算输入数据的方差
        - 4、对数据进行表缀化
        - 5、训练参数 gamma, beta
        - 6、输出y通过gamma, beta的线性变化得到新的值

        在正向传播中，通过可学习的gamma, beta参数求出新的分布的值
        在反向传播中，通过链式求导，求出gamma，beta及相关的权值
## ResNet


  