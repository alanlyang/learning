## 前馈神经网络
    定义：
        每个神经元只与前一层的神经元相连，接受前一层的输出，并且输入给下一层，隔层之间没有反馈
        全称 Feedforward Neural Netword, FNN, 简称前馈网络
        采用单向多层结果，整个网络无反馈，信号从输入层向输出层单向传播
    特点：
        网络结构简单，应用广泛
        能够一任意精度逼近任意连续函数及平方可积函数
        可以精确实现任意有限训练样本集

        从系统的观点：
        前馈网络是一种静态非线性映射， 通过简单的非线性处理单元的复合映射，可以 获得复杂的非线性处理能力
        从计算的观点：
        缺乏 丰富的动力学行为

        大部分前馈网络都是学习网络，其分类能力和模式识别能力一般都强于反馈网络
    
    常见前馈神经网络
        感知机网络：主要用于模式分类，也可以用在基于模式分类的学习控制和多模态控制中
        BP网络： 连接权调整使用反向传播学习算法的前馈网络，BP网络的神经元变化函数采用的Sigmoid函数，输出量是0-1之间的连续量，可实现输入到输出的任意非线性映射

## 全链接层： 线性变化 + 激活函数
    - 最常用和基础的层之一
    - 对输入矩阵A进行线性变化f(AW+b) + 激活函数的操作，f即为激活函数。如果不指定激活函数，则为纯线性变化
    - tf.keras.layers.Dense  参数如下
      - units L 输出tensor的维度
      - activation 激活函数
      - use_bias 是否使用偏置向量
      - kernel_initializer 和 bias_initializer 权重初始化器, 默认为glorot_uniform（截取的正态分布）
## 池化层
    池化层的目的：
        - 降低信息冗余
        - 提升模型的尺度不变形，旋转不变性
        - 防止过拟合
    常见类型
        - 最大池化池: 能够学习到图像的边缘和文立
        - 均值池化： 常见于SE模块以及分类模块中，优点在于可以减少估计均值的偏移，提升鲁棒性
        - 随机池化: 随机位置池化集合了随机池化和最大值池化
        - 中值池化： 基本很少见，参考的是图像处理中的中值滤波而引申的一中呢池化方式
        - 组合池化:
        - 分阶数池化： 见于pytorch
    简单解释：
        - 用某个区域内值的函数值来代替该区域
        - 如最大池化就是用区域内的最大值来代替该区域
    其他：
        在resNet之后，池化层在分类网络中应用主键变少，往往采用stride-2的卷积代替最大池化层

## 卷积层
    卷积层会对图像进行高维特征提取，使用的原理是数学中的卷积运算
    在tf.keras中主要使用conv2D实现。接受参数：
        - filters: 卷积层神经元数目
        - padding： same/valid,
          - 如果不使用padding,
              - 则每次卷积操作以后，图像都会变小，导致多次卷积后特征丢失
              - 边缘信息只被少量的卷积输出使用，导致许多边缘位置信息丢失
          - 使用valid,则卷积不进行边缘填充，图像经过卷积后会缩小
          - 使用same,则对边缘进行填充，默认填0
        - kernel_size： 感受野大小，
        - activation

## VGG16 网络
    - VGG 系列是由 Oxford 的Visual Geometry Group 提出的
    - 主要证明了增加网络深度能够在一定程度上影响网络的最终性能
    - 原理：
      - 和 AlexNet 相比，采用连续的几个3*3的卷积核代替AlexNet中的较大卷积核
      - 对于给定的感受野，采用堆积的小卷积核优于采用大的卷积核。多层非线性层可以增加网络深度来保证学习更复杂的模式，而且参数更少
    - 网络结构
      - VGG16 包含16个隐层（13个卷积层，3个全链接层）
      - VGG19 包含19个隐层（16个卷积层，3个全链接层）
      - VGG 网络从头到尾都是使用的3*3的卷积核和2*2的max pooling
    - 优缺点
      - 优点：
        -  结构简洁，整个网络都使用同样大小的卷积核尺寸(3*3)和最大池化层尺寸(2*2)
        -  几个小滤波器卷积层的组合比一个大滤波器卷积层号
        -  验证了通过不断加深网络结构可以提高性能
      - 缺点
        -  耗费更多的计算资源，并且使用了更多的参数，导致了很高的内存占用

## RNN（循环神经网络）

    - 全称 Recurrent Neural Netword
    - cnn和dense都只能处理单一一次样本的输入，学习单个样本内的信息，无法学习前一个输入与后一个输入之间的关系，因此出现了RNN
    - RNN中state其实是上个时刻隐层的权重矩阵
    - RNN和传统神经网络的主要却别就在于每次都会将前一次的输出结果带到下一次的隐层中，一起训练
    - RNN 是一个链式结构，每个时间片使用的是相同的参数
    - RNN 的输出仅由权值，偏置、激活 函数确定
    - RNN的缺点也比较明显，短期记忆的影响比较大，长期的记忆影响就很小
      - RNN有短期记忆问题，无法处理很长的输入序列
      - 训练RNN需要投入大量的成本，训练收敛较慢
      - RNN模型训练中经常会出现梯度消失或者梯度爆炸，这是由RNN的权重矩阵循环相乘导致，相同函数的多次组合会导致极端的非线性行为

## LSTM(长短期记忆网络)
    - 参考资料： https://easyai.tech/ai-definition/lstm/
    - 全称： Long Short Term Memory
    - 是RNN的改进版，具有记忆长短期信息的能力,是一种特殊的RNN
    - 相比于RNN， LSTM选择只保留重要信息
    - LSTM引入了门(gate)机制用于控制特征的流通和损失
    - LSTM由一系列的LSTMCell（LSTM unit组成）
    - 所有的递归神经网络都具有神经网络的链式重复模块，在标准RNN中，这个重复模块具有非常简单的结构，只有单个的tanh层
    - LSTM也具有类似的链式结构，但是重复模块具有不同的结构，不是一个单独的神经网络层，而是四个，并且以非常特殊的形式进行交互
  <img src="https://easy-ai.oss-cn-shanghai.aliyuncs.com/2019-07-05-lstm.png">
    - LSTM的核心部分是图中最上面的一条线，一般称为cell state, 贯穿整个LSTM链式系统
      - C(t) = 遗忘门*C(t-1) + 输入门* 单元状态更新值
      - 遗忘门是一个向量，向量的每个元素都在[0, 1]之间
      - LSTM最重要的门机制表示 遗忘门和C(t-1)之间的单位乘关系
      - 输入门i(t)也是一个向量，元素也都在[0,1]之间，经由sigmod计算而成
      - 单元状态更新值由tanh计算而成
      - 输出门 = o(t)*tanh(C(t))
    - 总结起来一个LSTMCell 由输入门、遗忘门，输出门，状态更新组成
  
## GRU
    - 全称 Gated Recurrent Unit 
    - 是LSTM的一个变种，主要在LSTM的基础上做了一些简化和调整，在训练集较大的情况下可以节省很多时间
    - GRU在LSTM的基础上，将忘记门和输入门合成了一个单一的更新门
      - 同样还混合了细胞状态和隐藏状态。
      - 比LSTM的模型要简单，也是非常流行的变体
  <img src="https://pic4.zhimg.com/80/v2-1838ebd696f1e4d16e41f1a126ff85a0_720w.jpg">
  
    标准LSTM和GRU的差别不大，但是都比tanh要明显好很多，在选择标准LSTM或者GRU的时候还要看具体的任务是什么
    使用LSTM的原因是解决RNN Deep Netword的Gradient错误累计太多，以至于梯度爆炸或者梯度消失
    GRU的构造比LSTM少一个gate， 因此计算时少几个矩阵相乘，在训练数据很大的情况下GRU能节省很多时间

## BN网络层
    - 全称 Batch Normalization 批量标准化
    - 计算步骤如下：
        - 1、输入数据
        - 2、计算输入数据的均值
        - 3、计算输入数据的方差
        - 4、对数据进行表缀化
        - 5、训练参数 gamma, beta
        - 6、输出y通过gamma, beta的线性变化得到新的值

        在正向传播中，通过可学习的gamma, beta参数求出新的分布的值
        在反向传播中，通过链式求导，求出gamma，beta及相关的权值
    - 主要是为了解决多次求导后的梯度消失或者梯度爆炸问题, 这就会导致网络中只有前几层或者后几层在学习，导致失去深度网络的意义
    - 输入：待进入激活函数的变量
    - 输出：
      - 1、对于K个激活函数前的数据，需要计算k个循环，每个循环中按照那个上述方法计算均值和方差，通过gamma和beta与输入x的变化求出BN层输出
      - 2、在反向传播时利用gamma和beta求的梯度从而改变训练权值
      - 3、通过不断迭代直到训练结束,得到gamma和beta，以及记录的均值和方差 
      - 4、在预测的正向传播时，使用训练时最后得到的gamma和beta，以及均值和方差的无偏估计

    - BN层实际上是一个归一化层，可以提高数据的泛化能力、；算法原理就是在每一层的输入时增加一个归一化层，然后再进入下一层 
  
## ResNet
      - 全称： Deep Residual Network  深度残差网络
      - 提出是CNN图像史上的里程碑事件
      - ResNet解决了深度CNN模型难训练的问题，相比VGG的19层， ResNet有152层
      - ResNet使用了残差学习
        - 从经验来看，随着模型深度的增加，网络可以进行更加复杂的特征模式提取，所以理论上可以取得更好的结果 
        - 但是实验发现你深度网络存在退化现象： 当网络深度增加时，网络准确度出现饱和，甚至出现下降
        - 由此引发思考，当通过堆积新层来建立网络时，极端情况下新层会完全复制上一层的参数，不起作用，但这样至少不应该出现网络的退化
      - 残差学习的提出，就是为了解决深层模型的退化问题。
      - 残差学习：
        - 当一个堆积层结构（几层堆积而成）的输入为X时，其学习到的特征记为H(x),现在我们希望学习到残差F(x) = H(x)-x
        - 这样原始的学习特征 H(x) = F(x)+x,
        - 残差学习比原始特征更加容易学习，当残差F(x)为0时，此时堆积层H(x)=x 只作恒等映射，至少网络性能不会下降
        - 事实上残差F(x)也不可能为0，因此会使新的堆积层在输入特征的基础上学习到新的特征，从而拥有更好的性能
        - 残差学习是一种短路连接（shortcut connection）
        - 每个残差单元一般包含多层结构
      - ResNet其实是在VGG19上进行了修改，并通过短路机制加入了残差单元
  <img src="https://pic2.zhimg.com/80/v2-7cb9c03871ab1faa7ca23199ac403bd9_720w.jpg">
        - 变化主要提现在ResNet直接使用stride=2的卷积做下采样， 并且使用了global average pool替换了全连接层
          - global averge pooling其实就是将pooling窗口设置为整个map大小
      - Resnet的设计原则：
        - 当feature map 大小降低到一半时，feature map的数量增加一倍，从而保持网络层的复杂度
          - feature map 卷积网络中每个卷积核产出一个feature map,
          - 事实上，在每一个卷积层上，有多个卷积核
        - 从图中可以看出 18层的使用的使两层间残差学习， 34层是哟就那个的三层间残差学习，三层卷积核分别是1*1， 3*3， 1*1
      - ResNet有两种残差单元，左为浅层，右为深层
  <img src="https://pic1.zhimg.com/80/v2-0892e5423616c30f69ded61111b111c0_720w.jpg">
        - 对于短路连接，当输入维度核输出一致时，可以直接想加。
        - 当不一致时：
          - 1、采用zero-padding增加维度，此时一般需要先做一个dowmstamp,可以采用stride=2的pooling
          - 2、采用新的映射，一般采用1*1的卷积
      - 论文最优的残差网络结构
  <img src="https://pic3.zhimg.com/80/v2-4e0bf37ecad2f306fe09d32a2d37d908_720w.jpg">
      改进前后一个明显的变化是采用pre-activation， BN和relu都提前了。

## GNN （图神经网络)
    - 全称 Graph Neural Networks 
    - 思想：

## GCN 

## DCN

## Wide&Deep

## ESSM

## ESMM

## MMoE