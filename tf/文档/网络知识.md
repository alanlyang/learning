## 前馈神经网络
    定义：
        每个神经元只与前一层的神经元相连，接受前一层的输出，并且输入给下一层，隔层之间没有反馈
        全称 Feedforward Neural Netword, FNN, 简称前馈网络
        采用单向多层结果，整个网络无反馈，信号从输入层向输出层单向传播
    特点：
        网络结构简单，应用广泛
        能够一任意精度逼近任意连续函数及平方可积函数
        可以精确实现任意有限训练样本集

        从系统的观点：
        前馈网络是一种静态非线性映射， 通过简单的非线性处理单元的复合映射，可以 获得复杂的非线性处理能力
        从计算的观点：
        缺乏 丰富的动力学行为

        大部分前馈网络都是学习网络，其分类能力和模式识别能力一般都强于反馈网络
    
    常见前馈神经网络
        感知机网络：主要用于模式分类，也可以用在基于模式分类的学习控制和多模态控制中
        BP网络： 连接权调整使用反向传播学习算法的前馈网络，BP网络的神经元变化函数采用的Sigmoid函数，输出量是0-1之间的连续量，可实现输入到输出的任意非线性映射

## 全链接层： 线性变化 + 激活函数
    - 最常用和基础的层之一
    - 对输入矩阵A进行线性变化f(AW+b) + 激活函数的操作，f即为激活函数。如果不指定激活函数，则为纯线性变化
    - tf.keras.layers.Dense  参数如下
      - units L 输出tensor的维度
      - activation 激活函数
      - use_bias 是否使用偏置向量
      - kernel_initializer 和 bias_initializer 权重初始化器, 默认为glorot_uniform（截取的正态分布）
## 池化层
    池化层的目的：
        - 降低信息冗余
        - 提升模型的尺度不变形，旋转不变性
        - 防止过拟合
    常见类型
        - 最大池化池: 能够学习到图像的边缘和文立
        - 均值池化： 常见于SE模块以及分类模块中，优点在于可以减少估计均值的偏移，提升鲁棒性
        - 随机池化: 随机位置池化集合了随机池化和最大值池化
        - 中值池化： 基本很少见，参考的是图像处理中的中值滤波而引申的一中呢池化方式
        - 组合池化:
        - 分阶数池化： 见于pytorch
    简单解释：
        - 用某个区域内值的函数值来代替该区域
        - 如最大池化就是用区域内的最大值来代替该区域
    其他：
        在resNet之后，池化层在分类网络中应用主键变少，往往采用stride-2的卷积代替最大池化层

## 卷积层
    卷积层会对图像进行高维特征提取，使用的原理是数学中的卷积运算
    在tf.keras中主要使用conv2D实现。接受参数：
        - filters: 卷积层神经元数目
        - padding： same/valid,
          - 如果不使用padding,
              - 则每次卷积操作以后，图像都会变小，导致多次卷积后特征丢失
              - 边缘信息只被少量的卷积输出使用，导致许多边缘位置信息丢失
          - 使用valid,则卷积不进行边缘填充，图像经过卷积后会缩小
          - 使用same,则对边缘进行填充，默认填0
        - kernel_size： 感受野大小，
        - activation

## VGG16 网络
    - VGG 系列是由 Oxford 的Visual Geometry Group 提出的
    - 主要证明了增加网络深度能够在一定程度上影响网络的最终性能
    - 原理：
      - 和 AlexNet 相比，采用连续的几个3*3的卷积核代替AlexNet中的较大卷积核
      - 对于给定的感受野，采用堆积的小卷积核优于采用大的卷积核。多层非线性层可以增加网络深度来保证学习更复杂的模式，而且参数更少
    - 网络结构
      - VGG16 包含16个隐层（13个卷积层，3个全链接层）
      - VGG19 包含19个隐层（16个卷积层，3个全链接层）
      - VGG 网络从头到尾都是使用的3*3的卷积核和2*2的max pooling
    - 优缺点
      - 优点：
        -  结构简洁，整个网络都使用同样大小的卷积核尺寸(3*3)和最大池化层尺寸(2*2)
        -  几个小滤波器卷积层的组合比一个大滤波器卷积层号
        -  验证了通过不断加深网络结构可以提高性能
      - 缺点
        -  耗费更多的计算资源，并且使用了更多的参数，导致了很高的内存占用