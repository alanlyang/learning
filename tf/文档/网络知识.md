## 前馈神经网络
    定义：
        每个神经元只与前一层的神经元相连，接受前一层的输出，并且输入给下一层，隔层之间没有反馈
        全称 Feedforward Neural Netword, FNN, 简称前馈网络
        采用单向多层结果，整个网络无反馈，信号从输入层向输出层单向传播
    特点：
        网络结构简单，应用广泛
        能够一任意精度逼近任意连续函数及平方可积函数
        可以精确实现任意有限训练样本集

        从系统的观点：
        前馈网络是一种静态非线性映射， 通过简单的非线性处理单元的复合映射，可以 获得复杂的非线性处理能力
        从计算的观点：
        缺乏 丰富的动力学行为

        大部分前馈网络都是学习网络，其分类能力和模式识别能力一般都强于反馈网络
    
    常见前馈神经网络
        感知机网络：主要用于模式分类，也可以用在基于模式分类的学习控制和多模态控制中
        BP网络： 连接权调整使用反向传播学习算法的前馈网络，BP网络的神经元变化函数采用的Sigmoid函数，输出量是0-1之间的连续量，可实现输入到输出的任意非线性映射

## 全链接层： 线性变化 + 激活函数
    - 最常用和基础的层之一
    - 对输入矩阵A进行线性变化f(AW+b) + 激活函数的操作，f即为激活函数。如果不指定激活函数，则为纯线性变化
    - tf.keras.layers.Dense  参数如下
      - units L 输出tensor的维度
      - activation 激活函数
      - use_bias 是否使用偏置向量
      - kernel_initializer 和 bias_initializer 权重初始化器, 默认为glorot_uniform（截取的正态分布）
## 池化层
    池化层的目的：
        - 降低信息冗余
        - 提升模型的尺度不变形，旋转不变性
        - 防止过拟合
    常见类型
        - 最大池化池: 能够学习到图像的边缘和文立
        - 均值池化： 常见于SE模块以及分类模块中，优点在于可以减少估计均值的偏移，提升鲁棒性
        - 随机池化: 随机位置池化集合了随机池化和最大值池化
        - 中值池化： 基本很少见，参考的是图像处理中的中值滤波而引申的一中呢池化方式
        - 组合池化:
        - 分阶数池化： 见于pytorch
    简单解释：
        - 用某个区域内值的函数值来代替该区域
        - 如最大池化就是用区域内的最大值来代替该区域
    其他：
        在resNet之后，池化层在分类网络中应用主键变少，往往采用stride-2的卷积代替最大池化层

## 卷积层
    卷积层会对图像进行高维特征提取，使用的原理是数学中的卷积运算
    在tf.keras中主要使用conv2D实现。接受参数：
        - filters: 卷积层神经元数目
        - padding： same/valid,
          - 如果不使用padding,
              - 则每次卷积操作以后，图像都会变小，导致多次卷积后特征丢失
              - 边缘信息只被少量的卷积输出使用，导致许多边缘位置信息丢失
          - 使用valid,则卷积不进行边缘填充，图像经过卷积后会缩小
          - 使用same,则对边缘进行填充，默认填0
        - kernel_size： 感受野大小，
        - activation

## VGG16 网络
    - VGG 系列是由 Oxford 的Visual Geometry Group 提出的
    - 主要证明了增加网络深度能够在一定程度上影响网络的最终性能
    - 原理：
      - 和 AlexNet 相比，采用连续的几个3*3的卷积核代替AlexNet中的较大卷积核
      - 对于给定的感受野，采用堆积的小卷积核优于采用大的卷积核。多层非线性层可以增加网络深度来保证学习更复杂的模式，而且参数更少
    - 网络结构
      - VGG16 包含16个隐层（13个卷积层，3个全链接层）
      - VGG19 包含19个隐层（16个卷积层，3个全链接层）
      - VGG 网络从头到尾都是使用的3*3的卷积核和2*2的max pooling
    - 优缺点
      - 优点：
        -  结构简洁，整个网络都使用同样大小的卷积核尺寸(3*3)和最大池化层尺寸(2*2)
        -  几个小滤波器卷积层的组合比一个大滤波器卷积层号
        -  验证了通过不断加深网络结构可以提高性能
      - 缺点
        -  耗费更多的计算资源，并且使用了更多的参数，导致了很高的内存占用

## RNN（循环神经网络）

    - 全称 Recurrent Neural Netword
    - cnn和dense都只能处理单一一次样本的输入，学习单个样本内的信息，无法学习前一个输入与后一个输入之间的关系，因此出现了RNN
    - RNN中state其实是上个时刻隐层的权重矩阵
    - RNN和传统神经网络的主要却别就在于每次都会将前一次的输出结果带到下一次的隐层中，一起训练
    - RNN 是一个链式结构，每个时间片使用的是相同的参数
    - RNN 的输出仅由权值，偏置、激活 函数确定
    - RNN的缺点也比较明显，短期记忆的影响比较大，长期的记忆影响就很小
      - RNN有短期记忆问题，无法处理很长的输入序列
      - 训练RNN需要投入大量的成本，训练收敛较慢
      - RNN模型训练中经常会出现梯度消失或者梯度爆炸，这是由RNN的权重矩阵循环相乘导致，相同函数的多次组合会导致极端的非线性行为

## LSTM(长短期记忆网络)
    - 参考资料： https://easyai.tech/ai-definition/lstm/
    - 全称： Long Short Term Memory
    - 是RNN的改进版，具有记忆长短期信息的能力,是一种特殊的RNN
    - 相比于RNN， LSTM选择只保留重要信息
    - LSTM引入了门(gate)机制用于控制特征的流通和损失
    - LSTM由一系列的LSTMCell（LSTM unit组成）
    - 所有的递归神经网络都具有神经网络的链式重复模块，在标准RNN中，这个重复模块具有非常简单的结构，只有单个的tanh层
    - LSTM也具有类似的链式结构，但是重复模块具有不同的结构，不是一个单独的神经网络层，而是四个，并且以非常特殊的形式进行交互
  <img src="https://easy-ai.oss-cn-shanghai.aliyuncs.com/2019-07-05-lstm.png">
    - LSTM的核心部分是图中最上面的一条线，一般称为cell state, 贯穿整个LSTM链式系统
      - C(t) = 遗忘门*C(t-1) + 输入门* 单元状态更新值
      - 遗忘门是一个向量，向量的每个元素都在[0, 1]之间
      - LSTM最重要的门机制表示 遗忘门和C(t-1)之间的单位乘关系
      - 输入门i(t)也是一个向量，元素也都在[0,1]之间，经由sigmod计算而成
      - 单元状态更新值由tanh计算而成
      - 输出门 = o(t)*tanh(C(t))
    - 总结起来一个LSTMCell 由输入门、遗忘门，输出门，状态更新组成
  
## GRU
    - 全称 Gated Recurrent Unit 
    - 是LSTM的一个变种，主要在LSTM的基础上做了一些简化和调整，在训练集较大的情况下可以节省很多时间
    - GRU在LSTM的基础上，将忘记门和输入门合成了一个单一的更新门
      - 同样还混合了细胞状态和隐藏状态。
      - 比LSTM的模型要简单，也是非常流行的变体
  <img src="https://pic4.zhimg.com/80/v2-1838ebd696f1e4d16e41f1a126ff85a0_720w.jpg">
  
    标准LSTM和GRU的差别不大，但是都比tanh要明显好很多，在选择标准LSTM或者GRU的时候还要看具体的任务是什么
    使用LSTM的原因是解决RNN Deep Netword的Gradient错误累计太多，以至于梯度爆炸或者梯度消失
    GRU的构造比LSTM少一个gate， 因此计算时少几个矩阵相乘，在训练数据很大的情况下GRU能节省很多时间

## BN网络层
    - 全称 Batch Normalization 批量标准化
    - 计算步骤如下：
        - 1、输入数据
        - 2、计算输入数据的均值
        - 3、计算输入数据的方差
        - 4、对数据进行表缀化
        - 5、训练参数 gamma, beta
        - 6、输出y通过gamma, beta的线性变化得到新的值

        在正向传播中，通过可学习的gamma, beta参数求出新的分布的值
        在反向传播中，通过链式求导，求出gamma，beta及相关的权值
    - 主要是为了解决多次求导后的梯度消失或者梯度爆炸问题, 这就会导致网络中只有前几层或者后几层在学习，导致失去深度网络的意义
    - 输入：待进入激活函数的变量
    - 输出：
      - 1、对于K个激活函数前的数据，需要计算k个循环，每个循环中按照那个上述方法计算均值和方差，通过gamma和beta与输入x的变化求出BN层输出
      - 2、在反向传播时利用gamma和beta求的梯度从而改变训练权值
      - 3、通过不断迭代直到训练结束,得到gamma和beta，以及记录的均值和方差 
      - 4、在预测的正向传播时，使用训练时最后得到的gamma和beta，以及均值和方差的无偏估计

    - BN层实际上是一个归一化层，可以提高数据的泛化能力、；算法原理就是在每一层的输入时增加一个归一化层，然后再进入下一层 
  
## ResNet
      - 全称： Deep Residual Network  深度残差网络
      - 提出是CNN图像史上的里程碑事件
      - ResNet解决了深度CNN模型难训练的问题，相比VGG的19层， ResNet有152层
      - ResNet使用了残差学习
        - 从经验来看，随着模型深度的增加，网络可以进行更加复杂的特征模式提取，所以理论上可以取得更好的结果 
        - 但是实验发现你深度网络存在退化现象： 当网络深度增加时，网络准确度出现饱和，甚至出现下降
        - 由此引发思考，当通过堆积新层来建立网络时，极端情况下新层会完全复制上一层的参数，不起作用，但这样至少不应该出现网络的退化
      - 残差学习的提出，就是为了解决深层模型的退化问题。
      - 残差学习：
        - 当一个堆积层结构（几层堆积而成）的输入为X时，其学习到的特征记为H(x),现在我们希望学习到残差F(x) = H(x)-x
        - 这样原始的学习特征 H(x) = F(x)+x,
        - 残差学习比原始特征更加容易学习，当残差F(x)为0时，此时堆积层H(x)=x 只作恒等映射，至少网络性能不会下降
        - 事实上残差F(x)也不可能为0，因此会使新的堆积层在输入特征的基础上学习到新的特征，从而拥有更好的性能
        - 残差学习是一种短路连接（shortcut connection）
        - 每个残差单元一般包含多层结构
      - ResNet其实是在VGG19上进行了修改，并通过短路机制加入了残差单元
  <img src="https://pic2.zhimg.com/80/v2-7cb9c03871ab1faa7ca23199ac403bd9_720w.jpg">
        - 变化主要提现在ResNet直接使用stride=2的卷积做下采样， 并且使用了global average pool替换了全连接层
          - global averge pooling其实就是将pooling窗口设置为整个map大小
      - Resnet的设计原则：
        - 当feature map 大小降低到一半时，feature map的数量增加一倍，从而保持网络层的复杂度
          - feature map 卷积网络中每个卷积核产出一个feature map,
          - 事实上，在每一个卷积层上，有多个卷积核
        - 从图中可以看出 18层的使用的使两层间残差学习， 34层是哟就那个的三层间残差学习，三层卷积核分别是1*1， 3*3， 1*1
      - ResNet有两种残差单元，左为浅层，右为深层
  <img src="https://pic1.zhimg.com/80/v2-0892e5423616c30f69ded61111b111c0_720w.jpg">
        - 对于短路连接，当输入维度核输出一致时，可以直接想加。
        - 当不一致时：
          - 1、采用zero-padding增加维度，此时一般需要先做一个dowmstamp,可以采用stride=2的pooling
          - 2、采用新的映射，一般采用1*1的卷积
      - 论文最优的残差网络结构
  <img src="https://pic3.zhimg.com/80/v2-4e0bf37ecad2f306fe09d32a2d37d908_720w.jpg">
      改进前后一个明显的变化是采用pre-activation， BN和relu都提前了。

## GNN （图神经网络)
    - 全称 Graph Neural Networks 
    - 思想：之前的深度学习主要关注例如文字的序列结构，图片的平面结构等，
      - 序列任务NLP领域常用RNN, Transformer、CNN 对数据进行Encoder
      - 关注平面结构的cv方向更多的使用CNN及其变种对数据进行Encoder
      - GNN 主要对非欧式空间的图结果进行操作
    - 分类：
  - <img src="https://pic2.zhimg.com/80/v2-95f73c5719cb222cc86eb1342640655f_720w.jpg">
      - GCN(Graph Convolutional Networks): 图卷积神经网络
        - 基于谱的GCN 
        - 基于空间的GCN
      - GAT(Graph Attention Networks): 基于注意力更新的图网络
      - 基于门控更新的图网络
      - 具有跳边的图网络

    - 过程：
      - 主要分为两个步骤： forward 和 backward. 和传统的神经网络类似，forward计算结果，backword计算梯度， 用于网络训练
        - forward过程：
          - F(w)根据当前父节点，相邻子节点，相连的边的特征及状态对当前父节点的状态进行更新，迭代t次，达到稳定状态
          - G(w)根据节点的当前状态，输出节点的output
        - backward过程的目的就在于梯度更新
    - 基于矩阵形式的GNN实现
      - 

## GCN 
  - 全称 （Graph Convolutional Network）图卷积神经网络
  - 图是一种数据格式，可以用于表示社交网络、通信网络、蛋白粉子网络等，图中的节点表示网络中的个体，边表示个体间的连接关系
  - 目前大多数图神经网络都使用图卷积神经网(GCNs),这些模型是可卷积的，因为滤波器参数仔图中所有位置或者一个局部位置都可以共享
  - 图的表示更多的是使用图论中的邻接矩阵的形式来建立定点和边建立相应关系的拓扑图
  - GCN的目标是要学习图G=(V,E)上的一个信号或特征的一个映射
  - 输入包括：
    - 每一个节点的特征描述，可以携程一个N*D的特征矩阵(N表示节点数，D表示输入的特征数)
    - 矩阵形式的图结构的特征表述，通常是以邻接矩阵的形式
  - 模型会产生一个节点级别的输出Z(一个 N*F的特征矩阵，其中F表示 每一个节点的输出特征数)
  - 卷积方式可以分为两种：
    - 谱卷积
    - 空间域卷积

## DCN
  - 全称 Deep Cross Netword
  - 将LR， MLP 和 Cross Net并联可以得到DCN
  - CrossNet是一个堆叠型王丽萍。该部分的初始输入是将f个(1,k）的特征组向量concat为一个(1, f*K)的向量

  每层的计算过程如下：
    输入向量和初始输入向量做Cartesian product得到（f*k, f*k）的矩阵，再重新投影成（1,k）向量，每层输出都包含输入向量
    cartesian product指笛卡尔乘积
  - <img src="https://pic4.zhimg.com/80/v2-3e97d59af6df3e7e859600f055e175eb_1440w.jpg">

## Wide&Deep
  - 将LR和MLP并联即可得到wide&deep模型，可同时学习一阶特征和高阶特征
  - 线性网络和深度网络并联
  - <img src="https://pic4.zhimg.com/80/v2-dd52399bde1c9f8cae487284606e3ae2_1440w.jpg">

## DeepFM
  - 将LR、MLP、Quadratic Layer并联可以得到DeepFM
  - 其中MLP和Quadratic Layer共享Group Embeding.
  - DeepFM现在是效率和效果上都表现不错的模型
  - <img src="https://pic2.zhimg.com/80/v2-c7878f2246a38411a57216b94485f0bf_1440w.jpg">

## xDeepFM
  - 将LR , MLP, 和 CIN并联起来可以得到xDeepFM
  - CIN也是一个堆叠型网络，该部分的初始输入是一个(f,k)的矩阵
  - 每层的计算过程：
    - 输入矩阵（Hi, k）和初始输入矩阵沿嵌入维度做Cartesian product得到(Hi, f, k)三维举证 ，然后投影成(Hi+1, k)矩阵
    - CIN最后一层： 将CIN中间层的 输出矩阵沿嵌入维度方向 做sum pooling得到（H1,1）(H2,1)...的向量，然后将这些向量concat起来作为CIN网络的输出
  - <img src="https://pic4.zhimg.com/80/v2-3297a00ff2a0014cfcbfaaf2700f8f64_1440w.jpg">

## CF召回
  - 全称 Collaborative filter 协同过滤
  - 分为 UserCF 和 ItemCF
  - UserCF主要计算两个用户之间得相似度，这里得相似度为兴趣相似度
    - 通常使用jaccard公式或者余弦距离来计算相似度
    - 主要分为三个步骤
      - 1、建立item-user得倒排表
      - 2、建立U*U得相似度矩阵
      - 3、计算用户u对物品i得感兴趣成都
    - 参数k： 表示与用户u兴趣最相近得k个用户列表 
      - 精度（准召率）：准确率和召回率与参数k不呈线性关系，但是选择合适k对于获取推荐系统高得精度比较重要
      - 流行度： k越大，UserCF推荐得物品会越热门
      - 覆盖率： k越大，流行度越大，覆盖率会相应得越小
    - 缺陷：
      - 随着用户数量 得增长，计算用户两两间得兴趣想速度得时间复杂度会越来越大，与用户数量得平方呈正比关系
      - 难以对推荐得结果做出令人信服得解释
  - itemCF主要通过计算用户得历史行为记录，来分析物品之间得相似度：
    - 如果喜欢物品A得用户大多数也喜欢用户B,那么认为A和B具有一定得相似度
    - 主要分为三个步骤
      - 1、建立 I * I 的全0矩阵
      - 2、建立用户-物品的兴趣列表
      - 3、计算共现次数赋值给 I*I 矩阵
      - 4、计算与物品i最相似的的k个物品
    - 参数k：与物品i最相似的K个物品的集合
      - 流行度： 随着k的增大，推荐结果的流行度会逐渐提高，但是到一定程度后，流行度不会再有明显变化
      - 覆盖率： k越大，覆盖率会相应的降低

## ALS召回
  - 全称： alternative leaset square （交替最小二乘法）
    - 交替二乘法的基本思想是：
      - 1、随见初始化矩阵Q
      - 2、将Q当做已知，对损失函数求导=0， 得到P的值
      - 3、得到矩阵P后，再将P当做已知，重新计算矩阵Q
      - 4、两个过程交替进行，直到误差到达可以接受的程度
    - 本质是矩阵分解，通过多次迭代获取隐层的矩阵权值分布

## LINE召回
  - 全称： Large-scale Information Network Embeding 大规模信息网络embedding
  - Line主要用来将大型的信息网络嵌入到低维的向量空间中。
    - 一阶相似性: 一条边连接的顶点之间的关系
    - 二阶相似性: 与一个顶点相连的两个顶点的关系


## Glove召回

## item2vec召回

## DSSM

## ESSM

## ESMM

## MMoE

## Attention


## Deep Neural Networks for YouTube Recommendations
  YoutubeNet 的优点：
    1、降低训练规模，取的是平均点击数K ， k远小于商品规模n 
    2、减少特征工程，YoutubeNet取消了商品特征的构造，根据商品的id， 利用DNN自动学习商品的Embedding特征，利用用户历史点击喜好来表达用户的特征

  YoutubeNet整体思想
    利用深度神经网络，根据用户的基础特征和上下文环境，来模拟矩阵分解的过程。
    我们知道矩阵分解需要有用户的特征矩阵和商品的特征矩阵，文中通过一个DNN来代替用户矩阵的生成过程
    即将DNN最后一层的输出近似的作为用户的特征
    YouTubeNet 将商品特征作为用户特征一部分（用户所在的上下文）一起参与训练

  YoutubeNet整体框架
    <img src="https://pic2.zhimg.com/80/v2-5feb59a3d9172cc738a8a9d6e9bc502d_1440w.jpg">
    主要分为 embedding + 基础特征， DNN , softmax 这三大模型

    1）embedding+基础特征
       基础特征就是需要人工特征的部分  
   <img src="https://pic3.zhimg.com/80/v2-c6fdfe56f511d2f9db4aae21a1a1eb82_1440w.jpg">
       以视频为例，将所有视频的id组成一个词典。根据视频词典生成embedding视频矩阵（这个怎么生成需要考虑下），一个id对应一个embedding向量
       用户的历史观看是一个变长的视频id序列，根据id序列和embeding视频矩阵获取用户历史观看的embeding向量序列，通过加权（求平均或者attention）将该embedding序列映射呈一个定长的watch vector， 作为网络输入的一部分

    2) DNN 
        这里论文给的是三层Relu层
   <img src="https://pic2.zhimg.com/80/v2-73edadef4a42b6fb4829a2c28cae9245_1440w.jpg">

    3) softmax层
        最后一个Relu层输出用户的向量U,假设视频个数有m个，每个video的embedding向量长度为d，那么视频矩阵为m*d维。通过用户向量u和每个video的embedding向量进行内积，过一遍softmax,得出用户对每个video的概率，取概率最大的N个作为recall

    4) YouTubeNet 实际上是将推荐问题作为一个大规模的多分类问题（有多少个video就有多少个类别）,在t时刻根据用户U和上下文环境C,在视频库V中准确的预测出用户将要观看视频的类别（其实就是观看哪一个）