# TFrecord
    简介：
        - tfrecord事google官方推荐的一种二进制数据格式，事google专门为tensorflow设计的一种数据格式
        - 理论上可以保存任何形式的信息
        - 整个文件由四部分组成
          - 文件长度信息 uint64 length
          - 长度校验码   uint32  masked_crc32_of_length
          - 数据        byte   data[length]
          - 数据校验码   uint32 masked_crc32_of_data

        - TFRecord内部包含多个tf.train.Example
          - tf.train.Example 是满足protocol buffer数据标准的实现
          - 一个Example消息体中包含了一些列的tf.train.feature属性
          - 每一个Feature都是一个key-value对，其中key为string, Value则有以下三种类型
            - 1、bytes_list： 可以存储string 和 byte 两种数据类型
            - 2、float_list: 可以存储float(float32) 与 double(float64)两种数据类型
            - 3、int64_list: 可以存储 bool , enum, int32, uint32, int64, uint64

    TFRecord的核心内容在于内部由一系列的Example，Example满足protocolbuf协议下的消息体
    在tensorflow中 tf.train.Example的定义如下
```java

        message Example{
            Features features = 1;
        }

        message Features{
            map<String, Feature> feature = 1;
        }

        message Feature{
            oneof kind{
                BytesList bytes_list = 1;
                FloatList float_list = 2;
                Int64List int64_list = 3;

            }
        }
```

    使用tfRecord的好处
    1、特别适合与tensorflow,专门为tensorflow打造
    2、能更好的利用内存，更加方便赋值和移动
    3、不需要单独的标签文件，理论上，能够保存所有信息


## 模型(Model)与层(layer)
    model和layer是keras的两个重要概念
    - layer 将各种计算流程和变量封装
    - model 进行各种层之间的组织和连接，封装为一个整体。
      - 在需要调用模型时，只需要使用y_pred = model(X)即可
      - keras中，model以类的形式呈现，可以通过继承 tf.kears.Model自定义模型
      - 继承类中需要重写__init__函数和 call(input)函数,此外还可以自定义方法
   ><img src=https://tf.wiki/_images/model.png>

## 池化层
    池化层的目的：
        - 降低信息冗余
        - 提升模型的尺度不变形，旋转不变性
        - 防止过拟合
    常见类型
        - 最大池化池: 能够学习到图像的边缘和文立
        - 均值池化： 常见于SE模块以及分类模块中，优点在于可以减少估计均值的偏移，提升鲁棒性
        - 随机池化: 随机位置池化集合了随机池化和最大值池化
        - 中值池化： 基本很少见，参考的是图像处理中的中值滤波而引申的一中呢池化方式
        - 组合池化:
        - 分阶数池化： 见于pytorch
    简单解释：
        - 用某个区域内值的函数值来代替该区域
        - 如最大池化就是用区域内的最大值来代替该区域
    其他：
        在resNet之后，池化层在分类网络中应用主键变少，往往采用stride-2的卷积代替最大池化层

## 强化学习
    我们所熟知的有监督学习是在带标签的已知训练数据上进行学习，得到一个是从数据特征到标签的映射（预测模型），进而预测新的数据实例所具有的标签   
    在强化学习中有两个新的概念
        - 智能体
        - 环境
    在强化学习中，智能体通过与环境的交互来学习策略，从而最大化自己在环境中的收益

    有监督学习是与统计理论密切挂念的学习类型，关注的是 预测   
    强化学习是与计算机算法（尤其是动态规划和搜索）有着深入关联 

## tensorflow的编程API
    - tensorflow-kernel： tensorflow Distributed Execution Engine 分布式编程引擎
    - low-level: Python, C++ ， java. Go 
    - mid-level: Layers, Datasets, Metrics
    - High-level: Estimators,  Keras(2.0官方唯一推荐)

## tensorflow的软件层次
| 层 | 功能 | 组件 |
| :--- | :--- | :--- |  
| 视图层 | 计算图可视化 | TensorBoard |
| 工作流层 | 数据集准备、存储、加载 | Keras/TF Slim |
| 计算图层 | 计算图构造与优化 | TensorFlow Core |
| 高维计算层 | 高维数组计算 | Eigen |
| 数值计算层 | 矩阵计算、卷积计算 | BLAS/cuDNN/cuRAND/cuBLAS |
| 网络层 | 通信 | gRPC/RDMA |
| 设备层 | 硬件 | CPU/GPU |

## tensorflow中batch的概念
    深度学习的优化算法，其实就是梯度下降。每次梯度下降参数更新有两种方式：
    这里的计算loss ，其实对应着代码里的tf.reduce_mean(loss)
    - 1、遍历全部数据集后，再计算Loss,更新参数
      - 计算量大，计算速度慢，需要更多的epoch
    - 2、每看一个样本，就计算一次loss，更新一次梯度，这个称为随机梯度下降。
      - 速度较快，但收敛性能较差，优于样本的随机性，可能造成目标函数震荡的比较剧烈
    - 3、取两者的折中，称为mini-batch gradient descent, 小批的梯度下降
      - 减少了随机性，并没有增加太大的计算量。
  
    调节batch-size对效果的影响
    1、随着batch-size的增大，处理相同数据量的速度变快
    2、随着batch-size的增大，达到相同精度所需要的epoch变多
    3、由于上述矛盾，batch_size在某个时候到达时间上的最优，在某个时候达到精度最优，需要取平衡
## 神经网络学习的本质
    学习数据集的分布

## pu loss
    首先右Pu learning (positive-unableled learning)
    即只有正样本和未标注的样本，以此进行分类学习。

    应用领域：
        1、检索， 从大量无标注样本中选取 特定的样本，如：人脸标注
        2、异常检测   包括inlier-based outlier检测 
        3、序列数据检测 负样本的分布随时间改变，这样传统的分类将不在合适 , pu只需要更新未标注样本，这样的花销更小。
            例如垃圾邮件检测，由于存在对抗，负样本的形式一直在改变，而非垃圾则一般相对稳定
    
    PU分类
    目前的PU的方法可分位两类：
        1、从 unlabeled数据识别出有效的负样本，然后利用负样本训练分类器
        2、全部将未标注的样本作为负样本训练，同时将负样本赋予一个较小的权重值

    第一种方法在识别有效负样本时比较依赖于经验，需要对相关业务北京有良好的理解；
    第二种方法则依赖于围标住样本权重的设置，如果loss不合适，将存在Biased误差。为了改进目前学习存在的偏差问题 ，无偏Pu learning被引入

    puLearning使用的前提是， unlabeled样本中positive样本特征分布与已知的positive样本分布一致