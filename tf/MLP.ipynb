{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1599793831204",
   "display_name": "Python 3.7.3 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow import data as tfdata\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow import losses\n",
    "from tensorflow.keras import initializers as init\n",
    "\n",
    "from tensorflow.keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLP(Multilayer Perceptron) 多重感知机 ，也可以称为『多层全连接网络』  \n",
    "这里使用Minst手写体数据集进行实验"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据获取及预处理\n",
    "使用tf.keras,datasets\n",
    "\n",
    "在tensorflow中，图像数据集的一种典型表示为\\[图像数目， 长， 宽， 色彩通道数\\] 的四维张量。  \n",
    "mnist 中图像的大小为 28*28， 灰度图像，训练集60000张， 测试集10000张  \n",
    "因此train_data为 \\[60000, 28, 28\\]这种形式  \n",
    "需要使用expand_dims给最后添加一维色彩通道  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTLoader():\n",
    "    def __init__(self):\n",
    "        mnist = keras.datasets.mnist\n",
    "        # MNIST 中的图像默认为 unit8(0-255)，需要归一化到0-1，并增加一维颜色通道\n",
    "        (self.train_data, self.train_label),(self.test_data, self.test_label) = mnist.load_data()\n",
    "        # axis=-1表示最后一维，维数可以简单理解为下标\n",
    "        self.train_data = np.expand_dims(self.train_data.astype(np.float32)/255.0, axis=-1)\n",
    "        self.test_data = np.expand_dims(self.test_data.astype(np.float32)/255.0, axis=-1)\n",
    "\n",
    "        self.train_label = self.train_label.astype(np.float32)\n",
    "        self.test_label = self.test_label.astype(np.float32)\n",
    "\n",
    "        self.num_train_data, self.num_test_data = self.train_data.shape[0], self.test_data.shape[0]\n",
    "    \n",
    "    def get_batch(self, batch_size):\n",
    "        # 从数据集中随机抽取batch_size个元素并返回\n",
    "        index = np.random.randint(0, self.num_train_data, batch_size)\n",
    "        # TODO 解释train_data[index, :]的含义，需要看数据格式,\n",
    "        return self.train_data[index, :], self.train_label[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "np.random.randint(low, high, size)   \n",
    "返回size个在low ~ high之间的数  \n",
    "满足均匀分布，原则上每个batch被取到的次数相同，直接对数据进行shuffle也可，不过比较消耗内存"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型构建MLP\n",
    "这里使用tf.keras.Model, 需要重写 __init__（）函数和 call()函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # flatten层将输入的矩阵压平: 将除第一维batch_size以外的平度展平\n",
    "        self.flatten = layers.Flatten()\n",
    "        # 两层全连接层\n",
    "        self.dense1 = layers.Dense(100, activation=tf.nn.relu)\n",
    "        # 第二层的单元个数为分类的个数0-9十个数字\n",
    "        self.dense2 = layers.Dense(10)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # 第一层处理\n",
    "        x = self.flatten(inputs)\n",
    "        # 经过两层的全连接层\n",
    "        x = self.dense1(x)\n",
    "        x = self.dense2(x)\n",
    "        # 输出值经过softmax转化为预测的概率值\n",
    "        outputs = tf.nn.softmax(x)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型训练\n",
    "需要先定义一些模型的超参数，例如batch_size, epoches, learning_rate等  \n",
    "其实全链接层的单元数也可以作为超参传入 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epoches = 5\n",
    "batch_size = 50\n",
    "learning_rate = 0.001\n",
    "\n",
    "# 实例化模型和数据读取类\n",
    "data_loader = MNISTLoader()\n",
    "mlp_model = MLP()\n",
    "\n",
    "# 实例化优化器，使用SGD\n",
    "optimizer = optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "# mlp_model.compile(optimizer=optimizer, loss=\"sparse_categorical_crossentropy\", metrics=tf.keras.metrics.SparseCategoricalAccuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "train_data: 60000\ntest_data: 10000\n"
    }
   ],
   "source": [
    "print(\"train_data: %s\" % data_loader.num_train_data)\n",
    "print(\"test_data: %s\" % data_loader.num_test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 迭代步骤\n",
    "- 从DataLoader中随机取batch\n",
    "- 将数据送入模型，计算模型的预测值\n",
    "- 将模型的预测值与真实值进行比较，计算Loss\n",
    "- 计算损失函数关于模型变量的导数\n",
    "- 将求出的导数出入优化器，使用apply_gradients更新参数以最小化损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "batch 0 \t loss: 2.455234\nbatch 500 \t loss: 0.168660\nbatch 1000 \t loss: 0.087254\nbatch 1500 \t loss: 0.071638\nbatch 2000 \t loss: 0.108226\nbatch 2500 \t loss: 0.153149\nbatch 3000 \t loss: 0.159102\nbatch 3500 \t loss: 0.238657\nbatch 4000 \t loss: 0.024842\nbatch 4500 \t loss: 0.115302\nbatch 5000 \t loss: 0.053679\nbatch 5500 \t loss: 0.004715\n"
    }
   ],
   "source": [
    "# 总计的batch数量,使用tf.keras.Dataset.from_tensor_slices时生成pipline,直接在pipline中指定batch_size即可\n",
    "num_batches = int(data_loader.num_train_data//batch_size * num_epoches)\n",
    "# 循环喂数据\n",
    "for batch_index in range(num_batches):\n",
    "    X, y = data_loader.get_batch(batch_size)\n",
    "    # 模型训练\n",
    "    with tf.GradientTape() as tape:\n",
    "        # 默认调用call函数\n",
    "        y_pred = mlp_model(X)\n",
    "        # 使用分类交叉熵函数作为损失函数(适合稀疏特征)\n",
    "        loss = losses.sparse_categorical_crossentropy(y_true=y, y_pred=y_pred)\n",
    "        # 每个分类预测的平均值\n",
    "        loss = tf.reduce_mean(loss)\n",
    "        if batch_index % 500 == 0:\n",
    "            print(\"batch %d \\t loss: %f\" % (batch_index, loss))\n",
    "    # 更新梯度\n",
    "    grads = tape.gradient(loss, mlp_model.variables)\n",
    "    optimizer.apply_gradients(grads_and_vars=zip(grads, mlp_model.variables))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf.keras中， 有两个交叉熵有关的损失函数  \n",
    "- categorical_crossentropy\n",
    "- sparse_categorical_croosentrop\n",
    "\n",
    "其中sparse的含义是，真实的标签值可以直接传入int类型的标签类别。\n",
    "等价于\n",
    "```python\n",
    "categorical_crossentropy(y_true=tf.one_hot(y,depth=tf.shape(y_pred)[-1]), y_pred=y_pred)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型的评估\n",
    "使用tf.keras.metrics来进行评估模型的效果  \n",
    "这里使用SparseCagtegoricalAccurary评估器来评估在测试集上的性能\n",
    "该评估器能够输出预测正确的样本数占总样本的比例  \n",
    "通过update_state()方法向评估器传入 y_pred 和 y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "test_acc: 0.974100\n"
    }
   ],
   "source": [
    "def acc():\n",
    "    # 定义检测指标对象\n",
    "    sparse_acc = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "    batch_num= int(data_loader.num_test_data//batch_size)\n",
    "\n",
    "    for batch_index in range(batch_num):\n",
    "        start_index, end_index = batch_index * batch_size, (batch_index+1)*batch_size\n",
    "        y_pred = mlp_model.predict(data_loader.test_data[start_index: end_index])\n",
    "        sparse_acc.update_state(data_loader.test_label[start_index:end_index], y_pred)\n",
    "    print(\"test_acc: %f\" % sparse_acc.result())\n",
    "acc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用keras 实现卷积神经网络\n",
    "相对MLP，卷积神经网络加入了一些卷积层和池化层。  \n",
    "这里的网络结构并不唯一，可以增加、删除调整CNN的网络结构和参数   \n",
    "以达到更好的性能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}