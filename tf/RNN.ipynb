{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1600064070221",
   "display_name": "Python 3.7.3 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow import data as tfdata\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow import losses\n",
    "from tensorflow.keras import initializers as init\n",
    "\n",
    "from tensorflow.keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN\n",
    "    - 全称： Recurrent Netural Netword  \n",
    "    - 是一种适宜于处理序列数据的NN  \n",
    "    - 广泛使用与语言模型，文本生成，机器翻译等  \n",
    "    - 是为了更好的处理时序信息而设计的\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里我们使用RNN来进行尼采风格文本的自动生成  \n",
    "本质其实预测一段英文文本的连续字母的概率分布  \n",
    "通过逐个输入一段长为seq_length的序列，输出这些序列连续的下一个字符的概率分布，  \n",
    "依次递归，生成接下来的字符，即可完成文本生成的任务\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义DataLoader\n",
    "\n",
    "以字符为单位进行编码   \n",
    "设字符种类数为num_chars，则每一个字符赋予一个0到nums_chars-1之间的唯一整数编号i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader():\n",
    "    def __init__(self):\n",
    "        # 获取相关数据,尼采相关的文本信息\n",
    "        path = tf.keras.utils.get_file('nietzsche.txt', origin='https://s3.amazonaws.com/text-datasets/nietzsche.txt')\n",
    "\n",
    "        with open(path, 'r', encoding=\"utf-8\") as fin:\n",
    "            self.raw_text = fin.read().lower()\n",
    "        # 获取所有的字符集\n",
    "        self.chars = sorted(list(set(self.raw_text)))\n",
    "        # 双向编码的字典\n",
    "        self.char_indices = dict((c, i) for i,c in enumerate(self.chars))\n",
    "        self.indices_char = dict((i, c) for i,c in enumerate(self.chars))\n",
    "        # 文本的编码信息\n",
    "        self.text = [self.char_indices[c] for c in self.raw_text]\n",
    "        \n",
    "    def get_batch(self, seq_length, batch_size):\n",
    "        \"\"\"seq_length为目标字符串的长度\n",
    "        \"\"\"\n",
    "        seq = []\n",
    "        next_char = []\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            index = np.random.randint(0, len(self.text)-seq_length)\n",
    "            # 最后的seq_length个字符\n",
    "            seq.append(self.text[index: index+seq_length])\n",
    "            next_char.append(self.text[index+seq_length])\n",
    "        # 返回最后seq_length的编码，一级最后一个字符\n",
    "        return np.array(seq), np.array(next_char) #[batch_size,  seq_length], [num_chars]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型实现\n",
    "\n",
    "首先在_\\_init__中实例化一个LSTMCell单元，以及一个线性变化用的全连接层。  \n",
    "首先对序列进行One-hot编码。变化后的序列tensor形状为\\[seq_length, num_chars\\]  \n",
    "然后初始化RNN单元状态，存入变量state中   \n",
    "接下来将序列从头到为依次送入RNN单元，即在t时刻，将上一个时刻t-1的RNN单元状态state和序列的第t个元素inputs\\[t, :\\]送入RNN单元，得到当前时刻的输出output和RNN单元状态。  \n",
    "取出RNN单元最后一次的输出，通过全连接层变化到nums_chars维，作为模型的输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(tf.keras.Model):\n",
    "    def __init__(self, num_chars, batch_size, seq_length):\n",
    "        super().__init__()\n",
    "        self.num_chars = num_chars\n",
    "        self.seq_length = seq_length\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # 初始化一个LSTMCell\n",
    "        self.cell = tf.keras.layers.LSTMCell(units=256)\n",
    "        # 初始化一个全连接层\n",
    "        self.dense = tf.keras.layers.Dense(units=self.num_chars)\n",
    "    \n",
    "    def call(self, inputs, from_logits=False):\n",
    "        # [batch_size, seq_length, num_chars]\n",
    "        inputs = tf.one_hot(inputs, depth=self.num_chars)\n",
    "        # 获得RNN的初始状态\n",
    "        state = self.cell.get_initial_state(batch_size=self.batch_size, dtype=tf.float32)\n",
    "        # \n",
    "        for t in range(self.seq_length):\n",
    "            # 通过当前输入和前一刻的状态，得到输出和当前时间状态\n",
    "            # 其实Output输出的为当前字符之后其他所有可能字符的概率分布\n",
    "            # 这里训练使用每个样本中第t个位置的上下文？\n",
    "            # state 其实是隐层的权重矩阵\n",
    "            output, state = self.cell(inputs[:, t, :], state)\n",
    "        logits = self.dense(output)\n",
    "\n",
    "        # from_logits 参数控制输出是否通过softmax进行归一化, False表示使用， true表示不使用，即表示是否直接使用原始输出值\n",
    "        if from_logits:\n",
    "            return logits\n",
    "        else:\n",
    "            return  tf.nn.softmax(logits)\n",
    "    \n",
    "    def predict(self, inputs, temperature=1.):\n",
    "        \"\"\"inputs: 输入\n",
    "           temperature： 分布控制参数\n",
    "        \"\"\"\n",
    "        batch_size, _ =  tf.shape(inputs)\n",
    "\n",
    "        # 调用训练好的模型进行预测，获取下一个字符的概率分布\n",
    "        logits = self(inputs, from_logits=True)\n",
    "        # 使用带temperature参数的 softmax函数 获得归一化的概率分布值\n",
    "        prob = tf.nn.softmax(logits/temperature).numpy()\n",
    "        # 在预测的概率分布上随机取样\n",
    "        result  = np.array([np.random.choice(self.num_chars, p=prob[i, :]) for i in range(batch_size.numpy())])\n",
    "        return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型超参"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_batches = 1000\n",
    "seq_length = 40\n",
    "batch_size = 50\n",
    "learning_rete = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练过程\n",
    "- 从DataLoader中随机一批训练数据\n",
    "- 将数据送入模型，计算模型的预测值 \n",
    "- 将预测值与真实值进行比较，计算损失函数loss\n",
    "- 计算损失函数关于模型变量的导数\n",
    "- 使用优化器更新参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 实例化读取数据\n",
    "data_loader = DataLoader()\n",
    "data_loader.indices_char.pop(53)\n",
    "data_loader.indices_char.pop(54)\n",
    "data_loader.indices_char.pop(55)\n",
    "data_loader.indices_char.pop(56)\n",
    "data_loader.char_indices.pop(\"ä\")\n",
    "data_loader.char_indices.pop(\"æ\")\n",
    "data_loader.char_indices.pop(\"é\")\n",
    "data_loader.char_indices.pop(\"ë\")\n",
    "# 实例化模型\n",
    "model = RNN(len(data_loader.chars), batch_size, seq_length)\n",
    "# 优化器\n",
    "optimizer = optimizers.Adam(learning_rate=learning_rete)\n",
    "\n",
    "for batch_index in range(num_batches):\n",
    "    # 从训练姐中随机取数据\n",
    "    X, y = data_loader.get_batch(seq_length, batch_size)\n",
    "    with tf.GradientTape() as tape:\n",
    "        # 自动调用call方法\n",
    "        y_pred = model(X)\n",
    "        # 选用交叉熵损失函数(其实这些可以自定义,交叉熵适用于分类问题, 带sparse可以使用one-hot类型)\n",
    "        loss = tf.keras.losses.sparse_categorical_crossentropy(y_pred=y_pred, y_true=y)\n",
    "        loss = tf.reduce_mean(loss)\n",
    "        if batch_index%200 == 0:\n",
    "            print(\"batch %d \\t loss: %f\" % (batch_index, loss))\n",
    "    grads = tape.gradient(loss, model.variables)\n",
    "    optimizer.apply_gradients(grads_and_vars=zip(grads, model.variables))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预测\n",
    "\n",
    "文本生成过程有一点需要特别注意，使用tr.argmax()来将最大概率的值作为预测的值对于文本预测有些绝对，会使得生成的文本失去丰富性。  \n",
    "因此采用np.random.choice()按照分布概率随机采样   \n",
    "同时加入temperature参数控制分布的形状  参数值越大则分布越平缓，丰富度越高。\n",
    "\n",
    "为此为RNN类增加predict方法, 见RNN 类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_, _ = data_loader.get_batch(seq_length, 1)\n",
    "\n",
    "# 设置四种不同的丰富度(temperature)\n",
    "for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
    "    X = X_\n",
    "    print(\"丰富度: %f\" % diversity)\n",
    "    for t in range(400):\n",
    "        # 预测下一个字符的编号\n",
    "        y_pred = model.predict(X, diversity)\n",
    "        print(data_loader.indices_char[y_pred[0]], end=\"\", flush=True)\n",
    "        # 将预测的字符接在输入X的末尾，并截断第一个字符，保证X的长度不变， 方便循环预测\n",
    "        X = np.concatenate([X[:, 1:], np.expand_dims(y_pred, axis=1)], axis=-1)\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}