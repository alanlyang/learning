{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1600228739947",
   "display_name": "Python 3.7.3 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "from tensorflow import data as tfdata\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow import losses\n",
    "from tensorflow.keras import initializers as init\n",
    "\n",
    "from tensorflow.keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 图执行模式 : tf.function\n",
    "tensorflow2.0 默认使用即时执行模式(Eager Execution) 这种模式能够带来灵活及易于调试的特性    \n",
    "但在特定的场合，如追求高性能或者部署模型时，依然希望使用tensorflow1.x中的图执行模式(Graph Execution), 将模型转化为高效的tensorflow图模型  \n",
    "\n",
    "tensorflow 2 提供了tf.function模块，结合AutoGraph机制，只需奥加入@tf.function修饰符，就可以将模型以图执行方式执行\n",
    "\n",
    "** 并不是任何函数都可以被@tf.function修饰 **  \n",
    "@tf.function 使用静态编译将函数内的代码转化成计算图   \n",
    "因此对函数内可使用的语句有一定的限制，且需要函数内的操作本身能够被构建为计算图。 \n",
    "建议在函数内值使用tf原生操作，不要使用过于复杂的python语句  \n",
    "函数参数只包括tensor或者numpy数组，并最好能够按照计算图的思想取构建函数  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNistLoader():\n",
    "    def __init__(self):\n",
    "        # 定义mnist对象\n",
    "        mnist = tf.keras.datasets.mnist\n",
    "        # 读取mnist数据\n",
    "        (self.train_data, self.train_label), (self.test_data, self.test_label) = mnist.load_data()\n",
    "        # mnist的数据为 28*28的0~255的值, 需要将图像转化为[张数， 宽度， 高度， 通道]， 并归一化\n",
    "        self.train_data = np.expand_dims(self.train_data.astype(np.float32)/255.0, axis=-1)\n",
    "        self.test_data = np.expand_dims(self.test_data.astype(np.float32)/255.0, axis=-1)\n",
    "\n",
    "        self.train_label = self.train_label.astype(np.float32)\n",
    "        self.test_label = self.test_label.astype(np.float32)\n",
    "\n",
    "        # 统计相关信息\n",
    "        self.num_train_data, self.num_test_data = self.train_data.shape[0], self.test_data.shape[0]\n",
    "    \n",
    "    def get_batch(self, batch_size):\n",
    "        # 从train_data中随机抽出batch_size个数据\n",
    "        index = np.random.randint(0, self.num_train_data, batch_size)\n",
    "        # np数组可以接受list索引，返回list索引对应的值, 原生数据不支持\n",
    "        return self.train_data[index, :], self.train_label[index]\n",
    "\n",
    "\n",
    "class CNN(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        # 继承父类的属性和方法\n",
    "        super().__init__()\n",
    "        # 定义网络层\n",
    "        self.conv1 = layers.Conv2D(\n",
    "            # 卷积层神经元（卷积核）数目, 默认使用kernel_initializer默认使用glorot_uniform进行初始化，简单来讲就是有32个不一样的卷积核，可以指定kernel_initializer\n",
    "            filters=32,\n",
    "            # 感受野大小\n",
    "            kernel_size=[5, 5],\n",
    "            # padding策略, same/vaild, same表示图片卷积后大小不变，valid表示卷积后图像减小（不对标远进行填充）\n",
    "            padding='same',\n",
    "            # 激活函数\n",
    "            activation=tf.nn.relu\n",
    "        )\n",
    "\n",
    "        self.pool1 = layers.MaxPool2D(\n",
    "            pool_size=[2,2],\n",
    "            strides=2\n",
    "        )\n",
    "\n",
    "        self.conv2 = layers.Conv2D(\n",
    "            filters=64,\n",
    "            kernel_size=[5,5],\n",
    "            padding=\"same\",\n",
    "            activation=tf.nn.relu\n",
    "        )\n",
    "\n",
    "        self.pool2 = layers.MaxPool2D(pool_size=[2,2], strides=2)\n",
    "        # TODO, 和Flatten的区别是什么\n",
    "        self.flatten = layers.Reshape(target_shape=(7*7*64, ))\n",
    "        self.dense1 = layers.Dense(units=1024, activation=tf.nn.relu)\n",
    "        self.dense2 = layers.Dense(units=10)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense1(x)\n",
    "        x = self.dense2(x)\n",
    "        output = tf.nn.softmax(x)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "29.30221199989319\n"
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# 定义超参\n",
    "num_batches = 400\n",
    "batch_size = 50\n",
    "learning_rate = 0.001\n",
    "\n",
    "# 实例化对象\n",
    "data_loader = MNistLoader()\n",
    "model = CNN()\n",
    "\n",
    "optimizer = optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "@tf.function\n",
    "def train_one_step(X, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = model(X)\n",
    "        loss = losses.sparse_categorical_crossentropy(y_true=y, y_pred=y_pred)\n",
    "        loss = tf.reduce_mean(loss)\n",
    "        # 这里使用tf内置的tf.print()方法， tf.function不支持Python原生的print\n",
    "        # if count%100 == 0:\n",
    "        #     tf.print(\"loss:\", loss)\n",
    "    grads = tape.gradient(loss,model.variables)\n",
    "    optimizer.apply_gradients(grads_and_vars=zip(grads, model.variables))\n",
    "\n",
    "start_time = time.time()\n",
    "count = 0\n",
    "for batch_index in range(num_batches):\n",
    "    X, y = data_loader.get_batch(batch_size)\n",
    "    train_one_step(X, y)\n",
    "    count += 1\n",
    "end_time = time.time()\n",
    "print(end_time-start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "运行400个batch进行测试，加入@tf.function后为29秒，不加为36秒  \n",
    "一般而言，当模型由较多小操作组成时，@tf.function会带来较大的提升效果  \n",
    "而当模型的操作数量较少时，但单一操作均很耗时时，@tf.function带来的性能的提升不会很大"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf.function的内在机制 \n",
    "当被@tf.function修饰的函数第一次被调用的时候，进行以下操作：  \n",
    "- 在即时模式关闭的环境下，代码依次运行，（只定义了计算节点，没有进行任何实质运算）\n",
    "- 使用AutoGraph将python控制流语句转化为tf计算图中的对应节点，如 if-->tf.cond   while/for ---> tf.while\n",
    "- 基于上面两步，建立函数内代码的计算图表示 \n",
    "- 运行一次计算图 \n",
    "- 基于函数的名字和输入的函数参数的类型生成一个哈希值，并将建立的计算图缓存到一个hash表中\n",
    "- 在被@tf.function修饰后的函数再次被调用时，根据***函数名***核输入的***函数参数类型***计算函数hash值，检查表中是否由对应计算图的缓存。如果有则直接使用，否则重建"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AutoGraph: 将Python控制流转化为tensorflow的计算图\n",
    "使用tf.AutoGraph 模块的底层api tf.autograph.to_code将函数转化为计算图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tf.Tensor(1, shape=(), dtype=int32) tf.Tensor(0, shape=(), dtype=int32)\ndef tf__square_if_positive(x):\n  do_return = False\n  retval_ = ag__.UndefinedReturnValue()\n  with ag__.FunctionScope('square_if_positive', 'square_if_positive_scope', ag__.ConversionOptions(recursive=True, user_requested=True, optional_features=(), internal_convert_user_code=True)) as square_if_positive_scope:\n\n    def get_state():\n      return ()\n\n    def set_state(_):\n      pass\n\n    def if_true():\n      x_1, = x,\n      x_1 = x_1 * x_1\n      return x_1\n\n    def if_false():\n      x = 0\n      return x\n    cond = x > 0\n    x = ag__.if_stmt(cond, if_true, if_false, get_state, set_state, ('x',), ())\n    do_return = True\n    retval_ = square_if_positive_scope.mark_return_value(x)\n  do_return,\n  return ag__.retval(retval_)\n\n"
    }
   ],
   "source": [
    "@tf.function\n",
    "def square_if_positive(x):\n",
    "    if x>0:\n",
    "        x = x*x\n",
    "    else:\n",
    "        x = 0\n",
    "    return x\n",
    "\n",
    "a = tf.constant(1)\n",
    "b = tf.constant(-1)\n",
    "\n",
    "print(square_if_positive(a), square_if_positive(b))\n",
    "print(tf.autograph.to_code(square_if_positive.python_function))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** AutoGraph起到的时类似编译器的功能，使用户能更加自然的使用python的控制流构建有条件循环， 而无需使用tensorflow的API进行构建***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 传统的tf.Session\n",
    "tensorflow 2.x 提供了tf.compat.v1模块用以支持tensorflow 1.x版本的api  \n",
    "keras的模型是同时可以兼容Eager和Graph模式的。  \n",
    "在图模式下，model(input_tensor)只需运行依次以完成图的建立操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    num_batches = int(data_loader.num_train_data // batch_size * num_epochs)\n",
    "    # 建立计算图\n",
    "    X_placeholder = tf.compat.v1.placeholder(name='X', shape=[None, 28, 28, 1], dtype=tf.float32)\n",
    "    y_placeholder = tf.compat.v1.placeholder(name='y', shape=[None], dtype=tf.int32)\n",
    "    y_pred = model(X_placeholder)\n",
    "    loss = tf.keras.losses.sparse_categorical_crossentropy(y_true=y_placeholder, y_pred=y_pred)\n",
    "    loss = tf.reduce_mean(loss)\n",
    "    train_op = optimizer.minimize(loss)\n",
    "    sparse_categorical_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "    # 建立Session\n",
    "    with tf.compat.v1.Session() as sess:\n",
    "        sess.run(tf.compat.v1.global_variables_initializer())\n",
    "        for batch_index in range(num_batches):\n",
    "            X, y = data_loader.get_batch(batch_size)\n",
    "            # 使用Session.run()将数据送入计算图节点，进行训练以及计算损失函数\n",
    "            _, loss_value = sess.run([train_op, loss], feed_dict={X_placeholder: X, y_placeholder: y})\n",
    "            print(\"batch %d: loss %f\" % (batch_index, loss_value))\n",
    "\n",
    "        num_batches = int(data_loader.num_test_data // batch_size)\n",
    "        for batch_index in range(num_batches):\n",
    "            start_index, end_index = batch_index * batch_size, (batch_index + 1) * batch_size\n",
    "            y_pred = model.predict(data_loader.test_data[start_index: end_index])\n",
    "            sess.run(sparse_categorical_accuracy.update_state(y_true=data_loader.test_label[start_index: end_index], y_pred=y_pred))\n",
    "        print(\"test accuracy: %f\" % sess.run(sparse_categorical_accuracy.result()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}