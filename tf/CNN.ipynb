{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1599876976580",
   "display_name": "Python 3.7.3 64-bit ('anaconda3': virtualenv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用keras实现卷积神经网络\n",
    "这里仍然使用mnist数据   \n",
    "通过tf.keras.Model来自定义网络结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow import data as tfdata\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow import losses\n",
    "from tensorflow.keras import initializers as init\n",
    "\n",
    "from tensorflow.keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNistLoader():\n",
    "    def __init__(self):\n",
    "        # 定义mnist对象\n",
    "        mnist = tf.keras.datasets.mnist\n",
    "        # 读取mnist数据\n",
    "        (self.train_data, self.train_label), (self.test_data, self.test_label) = mnist.load_data()\n",
    "        # mnist的数据为 28*28的0~255的值, 需要将图像转化为[张数， 宽度， 高度， 通道]， 并归一化\n",
    "        self.train_data = np.expand_dims(self.train_data.astype(np.float32)/255.0, axis=-1)\n",
    "        self.test_data = np.expand_dims(self.test_data.astype(np.float32)/255.0, axis=-1)\n",
    "\n",
    "        self.train_label = self.train_label.astype(np.float32)\n",
    "        self.test_label = self.test_label.astype(np.float32)\n",
    "\n",
    "        # 统计相关信息\n",
    "        self.num_train_data, self.num_test_data = self.train_data.shape[0], self.test_data.shape[0]\n",
    "    \n",
    "    def get_batch(self, batch_size):\n",
    "        # 从train_data中随机抽出batch_size个数据\n",
    "        index = np.random.randint(0, self.num_train_data, batch_size)\n",
    "        # np数组可以接受list索引，返回list索引对应的值, 原生数据不支持\n",
    "        return self.train_data[index, :], self.train_label[index]\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义CNN模型\n",
    "和MLP相比，只是新加入了卷积层和池化层，这里的网络结构并不唯一，可以增加、删除、调整CNN的网络结构和参数，以达到更好的效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        # 继承父类的属性和方法\n",
    "        super().__init__()\n",
    "        # 定义网络层\n",
    "        self.conv1 = layers.Conv2D(\n",
    "            # 卷积层神经元（卷积核）数目, 默认使用kernel_initializer默认使用glorot_uniform进行初始化，简单来讲就是有32个不一样的卷积核，可以指定kernel_initializer\n",
    "            filters=32,\n",
    "            # 感受野大小\n",
    "            kernel_size=[5, 5],\n",
    "            # padding策略, same/vaild, same表示图片卷积后大小不变，valid表示卷积后图像减小（不对标远进行填充）\n",
    "            padding='same',\n",
    "            # 激活函数\n",
    "            activation=tf.nn.relu\n",
    "        )\n",
    "\n",
    "        self.pool1 = layers.MaxPool2D(\n",
    "            pool_size=[2,2],\n",
    "            strides=2\n",
    "        )\n",
    "\n",
    "        self.conv2 = layers.Conv2D(\n",
    "            filters=64,\n",
    "            kernel_size=[5,5],\n",
    "            padding=\"same\",\n",
    "            activation=tf.nn.relu\n",
    "        )\n",
    "\n",
    "        self.pool2 = layers.MaxPool2D(pool_size=[2,2], strides=2)\n",
    "        # TODO, 和Flatten的区别是什么\n",
    "        self.flatten = layers.Reshape(target_shape=(7*7*64, ))\n",
    "        self.dense1 = layers.Dense(units=1024, activation=tf.nn.relu)\n",
    "        self.dense2 = layers.Dense(units=10)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense1(x)\n",
    "        x = self.dense2(x)\n",
    "        output = tf.nn.softmax(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义超参和实例化对象\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义超参\n",
    "batch_size = 50\n",
    "epoch = 5\n",
    "learning_rate = 0.001\n",
    "\n",
    "# 实例化数据和模型对象\n",
    "mnist = MNistLoader()\n",
    "cnn = CNN()\n",
    "\n",
    "# \n",
    "optimizer = optimizers.Adam(learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "batch 0 \t loss: 2.305305\nbatch 500 \t loss: 0.015411\nbatch 1000 \t loss: 0.003332\nbatch 1500 \t loss: 0.003056\nbatch 2000 \t loss: 0.044779\nbatch 2500 \t loss: 0.002426\nbatch 3000 \t loss: 0.002162\nbatch 3500 \t loss: 0.035119\nbatch 4000 \t loss: 0.006237\nbatch 4500 \t loss: 0.001408\nbatch 5000 \t loss: 0.029309\nbatch 5500 \t loss: 0.001738\n"
    }
   ],
   "source": [
    "# num_batches \n",
    "num_batches = int(mnist.num_train_data*epoch//batch_size)\n",
    "# 循环喂数据\n",
    "for batch_index in range(num_batches):\n",
    "    X, y = mnist.get_batch(batch_size=batch_size)\n",
    "    # 记录梯度信息\n",
    "    with tf.GradientTape() as tape:\n",
    "        # 默认调用call方法\n",
    "        y_pred = cnn(X)\n",
    "        # 计算Loss\n",
    "        loss = tf.losses.sparse_categorical_crossentropy(y_pred=y_pred, y_true=y)\n",
    "        # 每个类别的误差平均值\n",
    "        loss = tf.reduce_mean(loss)\n",
    "        if batch_index % 500 == 0:\n",
    "            print(\"batch %d \\t loss: %f\" %(batch_index, loss))\n",
    "    # 梯度更新\n",
    "    grads = tape.gradient(loss, cnn.variables)\n",
    "    optimizer.apply_gradients(grads_and_vars=zip(grads, cnn.variables))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 验证指标\n",
    "这里还是使用sparse_acc来进行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "test_acc: 0.991600\n"
    }
   ],
   "source": [
    "# # 定义指标对象\n",
    "# sparse_acc = tf.keras.metrics.SparseCategoricalCrossentropy()\n",
    "\n",
    "# # 使用测试集进行验证\n",
    "# batch_num = int(mnist.num_test_data / batch_size)\n",
    "# for batch_index in range(batch_num):\n",
    "#     start_index, end_index = batch_index*batch_size, (batch_index+1)*batch_size\n",
    "#     y_pred = cnn.predict(mnist.test_data[start_index:end_index])\n",
    "#     # 更新\n",
    "#     # sparse_acc , y_true是数值， Y_pred为概率，如果y_true对应位置的值为y_pred对应的最大值的索引，则预测正确\n",
    "#     # 如 y_true = [[2, 1]] y_pred=[[0.5, 0.4, 0.1], [0.9, 0.05, 0.05]] 则 acc=1/2\n",
    "#     sparse_acc.update_state(mnist.test_label[start_index:end_index], y_pred)\n",
    "# print(sparse_acc.weights)\n",
    "# print(\"test_acc: %f\" % sparse_acc.result())\n",
    "\n",
    "def acc():\n",
    "    # 定义检测指标对象\n",
    "    sparse_acc = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "    batch_num= int(mnist.num_test_data//batch_size)\n",
    "\n",
    "    for batch_index in range(batch_num):\n",
    "        start_index, end_index = batch_index * batch_size, (batch_index+1)*batch_size\n",
    "        y_pred = cnn.predict(mnist.test_data[start_index: end_index])\n",
    "        sparse_acc.update_state(mnist.test_label[start_index:end_index], y_pred)\n",
    "    print(\"test_acc: %f\" % sparse_acc.result())\n",
    "acc()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以发现，相对与MLP，准确率有很大的提升.  \n",
    "事实上，通过改变网络结构（比如加入dropout层防止过拟合）准确率还有进一步的提高"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用keras中预定义的经典卷积神经网络结构\n",
    "\n",
    "tf.keras.applications中有一些预定义号的经典卷积神经网络结构,  \n",
    "如VGG16、 VGG19、ResNet、MobelNet等。  \n",
    "可以直接调用这些经典的卷积网络，甚至可以加载于训练的参数，而无需手动定义网络结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Downloading data from https://github.com/JonathanCMitchell/mobilenet_v2_keras/releases/download/v1.1/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224.h5\n14540800/14536120 [==============================] - 39s 3us/step\n"
    }
   ],
   "source": [
    "# 获取模型参数\n",
    "model = tf.keras.applications.MobileNetV2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当执行上述代码时，tf会自动下载预训练的参数。  \n",
    "也可以通过设置weights参数为None来随机初始化变量。  \n",
    "\n",
    "每个网络都有自己的详细参数设置，一些共同的参数如下：\n",
    "\n",
    "- input_shape: 输入tensor的形状， 大多默认为244*244*3。一般下限为 32\\*32或75\\*75\n",
    "- include_top: 网络的最后是否包含全链接层，默认为true\n",
    "- weights: 预训练权值，默认为\"imageNet\", 即载入当前模型在imageNet数据集上预训练的权值，需要随机初始化可以设为None\n",
    "- classes: 分类数，默认为1000. 修改参数需要include_top参数为true，且weights参数为None\n",
    "\n",
    "各个网络的参数可以参见keras文档: https://keras.io/applications/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}