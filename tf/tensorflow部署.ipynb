{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型的保存\n",
    "使用SaveModel即可导出tensorflow程序的完整信息。  \n",
    "相比与checkpoint只导出对应的参数举证， SaveModel在保留参数矩阵的同时，还保留了了计算图  \n",
    "当模型导出为SaveModel文件时，无需模型的源代码即可再次运行模型   \n",
    "这种特性使SaveModel尤其适用于模型的分享和部署。\n",
    "\n",
    "Keras模型均可方便的导出为SaveModel格式。  \n",
    "需要注意的是，因为SaveModel基于计算图，所以对于使用继承tf.keras.Model类建立的模型，其需要导出到SavedModel格式的方法（如call）都需要使用@tf.function来进行修饰.\n",
    "\n",
    "```python\n",
    "    tf.saved_model.save(model,\"目录\")\n",
    "    tf.saved_model.load(\"目录\")\n",
    "```\n",
    "\n",
    "对于使用继承tf.keras.Model类建立的Keras模型model, 使用SavedModel载入后无法使用model()进行推断，需要使用model.call()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 服务端模型部署 Tensorflow Serving\n",
    "模型训练完成后，需要将模型在生产环境中部署  \n",
    "在不考虑高并发和性能，使用Flask等Python下的web框架即可轻松实现服务器API  \n",
    "生产环境下，Tensorflow提供了Tensorflow Serving 来灵活且高性能的部署机器学习模型  \n",
    "\n",
    "使用以下命令即可部署  \n",
    "tensorflow_model_server \\  \n",
    "    -- reset_api_port=端口号 \\  \n",
    "    -- model_name=模型名 \\  \n",
    "    -- model_base_path=\"SavedModel格式模型的文件夹绝对地址（不含版本号）\"\n",
    "\n",
    "tensorflow serving 支持 gRPC和RESTful API 调用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 移动端模型部署 Tensorflow Lite\n",
    "tensorflow Lite是tensorflow在移动和IOT等边缘设备端的解决方案，提供了Java、Python和C++ API库  \n",
    "目前TF lite 只提供了推理功能，在服务端进行训练后，经过简单处理即可部署到边缘设备上   \n",
    "\n",
    "- 模型转换 \n",
    "- 边缘设备部署  \n",
    "\n",
    "### 模型转换\n",
    "转换工具有两种： 命令行工具、 Python API    \n",
    "TF2.0 推荐使用python API ， 命令行工具只提供了基本的转化功能  \n",
    "转换后的原模型为FlatBuffers 格式。     \n",
    "FlatBuffers 原来是 google 为了高性能场景创建的序列化库，相比 protocol Buffer 有更高的性能和更小的大小  \n",
    "\n",
    "转换方式有两种： Float格式 和 Quantized 格式\n",
    "\n",
    "命令行  \n",
    "    tflite_convert(终端执行)  \n",
    "    -- saved_model_dir=『原模型目录』 \\  \n",
    "    -- output_file=\"输出模型目录\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow in JavaScript(tensorflow的JS版本) \n",
    "在浏览器中使用tensorflow.js  \n",
    "使用本地资源来进行机器学习运算  \n",
    "\n",
    "相比与服务器端来讲的优点：  \n",
    "- 不用安装软件或驱动\n",
    "- 通过浏览器进行更加方便的人机交互\n",
    "- 通过手机浏览器调用手机硬件的各种传感器 \n",
    "- 用户数据无需上传服务器，在本地即可完成所需操作  \n",
    "\n",
    "浏览器引入：  \n",
    "```html\n",
    "<script src=\"http://unpkg.com/@tensorflow/tfjs/dist/tf.min.js\"></script>\n",
    "```\n",
    "\n",
    "tf.js不仅支持模型的推理，还支持模型的训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}